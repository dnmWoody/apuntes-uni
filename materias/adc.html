<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ADC - Arquitectura de Computadoras</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <link rel="stylesheet" href="styleM.css">
</head>
<body>
    <div class="container">
        <aside class="sidebar" id="sidebar">
            <h2>Contenido</h2>
            <ul>
                <li><a href="#interrupciones" onclick="hideMenu()">Interrupciones</a></li>
                <li><a href="#microcontroladores" onclick="hideMenu()">Microcontroladores</a></li>
                <li><a href="#pipeline" onclick="hideMenu()">Pipeline</a></li>
                <li><a href="#entrada-salida" onclick="hideMenu()">Entrada - Salida</a></li>
                <li><a href="#paralelismo" onclick="hideMenu()">Paralelismo</a></li>
                <li><a href="#secuenciadores" onclick="hideMenu()">Secuenciadores</a></li>
                <li><a href="#memoria-cache" onclick="hideMenu()">Memoria Caché</a></li>
                <li><a href="#coherencia-cache" onclick="hideMenu()">Coherencia Caché</a></li>
                <li><a href="#examenes" onclick="hideMenu()">Exámenes</a></li>
                <li><a href="../index.html" onclick="hideMenu()" style="color: rgb(207, 89, 89);">← Volver atrás</a></li>
            </ul>
            <button class="theme-toggle" onclick="toggleTheme()">Cambiar Tema</button>
        </aside>

        <!-- Botón para mostrar el menú cuando esté oculto en modo responsive -->
        <button id="show-menu-btn" class="show-menu-btn" onclick="toggleMenu()">Menú</button>
        <!--<button id="show-atras-btn" class="show-atras-btn"><a href="../index.html">Volver atrás</a></button>-->


        <main class="main-content" id="main-content">
            <!-- ********************************************************************************************************** INTERRUPCIONES -->
            <section id="interrupciones" class="contenido">
                <h2 class="titulo2">Interrupciones</h2>
                <p>Las interrupciones son cambios en el flujo de control causados no por el programa en ejecución, sino por eventos externos al mismo, como los relacionados con operaciones de E/S (Entrada/Salida).<br>
                    Por ejemplo, un disco duro que realiza una transferencia puede interrumpir el flujo normal de trabajo cuando finaliza la operación. Otro ejemplo es el teclado: al presionar una tecla, se genera una interrupción.<br>
                    Cuando se habla de interrupciones, se hace referencia a la detención temporal del flujo normal de ejecución del procesador, transfiriendo el control a la unidad manejadora de interrupciones, que se encargará de realizar la acción apropiada.<br>
                    Una vez finalizada la acción, el control debe ser devuelto al proceso interrumpido, restaurando los registros y el estado interno del sistema tal como estaban antes de la interrupción.<br>
                    Las interrupciones son asíncronas, lo que significa que pueden ocurrir en cualquier momento, ya que no son controladas directamente por el programa en ejecución.
                </p>
                <br>

                <h3>Mecanismo de interrupciones (Paso a Paso)</h3>
                <ol>
                    <li><b>El dispositivo genera una interrupción:</b> 
                        <br>Un dispositivo, como el teclado o un disco duro, necesita atención de la CPU, por lo que envía una señal de interrupción a través del bus del sistema, que es la conexión física entre los componentes de la computadora.</li>
                    <li><b>La CPU está lista para manejar la interrupción:</b>
                        <br> La CPU sigue trabajando en sus tareas hasta que pueda atender la interrupción. Una vez que esté lista, envía una señal de reconocimiento para decirle al dispositivo que la interrupción ha sido registrada.</li>
                    <li><b>Identificación del dispositivo que generó la interrupción:</b>
                        <br>El dispositivo coloca un número, llamado <b>vector de interrupción</b>, en el bus de datos. Este número identifica qué dispositivo solicitó la interrupción.</li>
                    <li><b>La CPU toma el vector de interrupción: </b>
                        <br> La CPU guarda temporalmente ese número (el vector de interrupción), que servirá para localizar la rutina que debe ejecutarse para manejar la interrupción.</li>
                    <li><b>La CPU guarda su estado actual: </b> 
                        <br> La CPU necesita recordar en qué punto estaba ejecutando el programa cuando ocurrió la interrupción. Para ello, guarda el <b>contador de programa</b> (que le dice qué instrucción estaba ejecutando) y el <b>estado del programa</b> (valores de los registros y otros datos importantes) en la pila, una zona de la memoria.</li>
                    <li><b>La CPU localiza la rutina de manejo de interrupciones: </b>
                        <br>Usando el vector de interrupción, la CPU busca una tabla especial en la memoria (ubicada en la parte baja de la memoria) que le indica dónde está la <b>rutina de servicio de interrupción</b>, que es el código que se va a ejecutar para manejar la interrupción.</li>
                    <li><b>Guardado de registros: </b>
                        <br>La rutina de servicio de interrupción guarda todos los registros (información importante que estaba usando la CPU), para poder restaurarlos después de atender la interrupción. Estos se guardan en la pila o en una tabla del sistema.</li>
                    <li><b>Identificación específica del dispositivo: </b>
                        <br>A veces, varios dispositivos pueden compartir el mismo vector de interrupción. En este caso, la CPU debe leer un registro en el dispositivo para saber cuál de ellos exactamente generó la interrupción (por ejemplo, si fue el teclado o un ratón).</li>
                    <li><b> Lectura de la información sobre la interrupción: </b>
                        <br>La CPU recoge toda la información adicional que necesita sobre el evento que causó la interrupción, como códigos de error o datos específicos del dispositivo.</li>
                    <li><b>Manejo de errores de E/S: </b>
                        <br>Si hubo algún error durante la operación de entrada/salida (E/S), como un problema al leer del disco duro, la rutina de servicio se encarga de solucionarlo aquí.</li>
                    <li><b>Actualización de variables: </b>
                        <br>Si la interrupción requiere que se actualicen contadores o variables globales, como el número de datos procesados, esto se hace en este paso.</li>
                    <li><b>Notificación al dispositivo: </b>
                        <br>Una vez procesada la interrupción, la CPU puede enviar un código al dispositivo para indicarle que la interrupción ha sido manejada correctamente.</li>
                    <li><b>Restauración del estado previo: </b>
                        <br>Se restauran los registros que fueron guardados para que la CPU vuelva exactamente al estado en el que estaba antes de la interrupción.</li>
                    <li><b>Reaundación del trabajo: </b>
                        <br>Finalmente, la CPU ejecuta una instrucción especial que la devuelve al estado anterior y continúa ejecutando el programa desde donde fue interrumpido.</li>
                </ol>
                
                <p>
                    Este proceso ocurre muy rápido, casi de manera instantánea, y por eso se puede, por ejemplo escribir en un teclado, seguir escribiendo sin notar ningún retraso. 
                </p>
                <br>

                <h3>Interrupciones Anidadas</h3>
                <p>
                    Las interrupciones anidadas permite que una interrupción de baja prioridad sea interrumpida por otra de alta prioridad, creando así una pila de interrupciones donde cada nivel se guarde su propio estado.<br>
                    Este mecanismo permite que los eventos más críticos puedan ser atendidos de inmediato. <br>
                    Cuando una ocurre la interrupción durante el manejo de otra interrupción, el estado de la CPU (registros, contador de programa y la palabra del estado del programa), se guarda en una estructura de pila. Esta pila sigue el principio de LIFO (Last In, First Out), lo que significa que la ultima interrupción en ser atendida es la primera en ser completada.
                </p>
                <br>

                <h3>Interrupciones Secuenciales</h3>
                <p>
                    Las interrupciones secuenciales se basa en manejar una interrupción a la vez antes de atender la siguiente, completando la ISR (Interrupt Service Routine).<br>
                    Aunque este método es simple, puede existir ciertas desventajas ya que existen interrupciones de diferentes prioridades. <br>
                    En estos casos, el sistema debe esperar que se complete la interrupción actual antes de antender la siguiente, lo que puede resultar demoras significativas si una interrupción de alto prioridad ocurre mientras se está manejando una interrupción de baja prioridad.<br>
                    Esto puede generar efectos negativos en la capacidad de respuesta del sistema y su desempeño general, especialmente en aplicaciones de tiempo real donde la rapidez es crítica.
                </p>
                <br>

                <h2>Ampliación de conceptos: Prioridades, Rutinas de Servicio (ISR), su ubicación y acceso a ellas</h2>
                <h3>Prioridades</h3>
                <p>
                    En un sistema de interrupciones, es crucial tener un mecnaismo para poder priorizar y manejar interrupciones mas urgentes. Existen varias alternativas para la gestión de interrupciones: 
                    <ul class="lista">
                        <li> <b>Interrupciones anidadas:</b> Permiten que una interrupción sea interrumpida por otra de mayor gravedad. Esto se logra guardadno el estado de la interrupción actual y atendiendo primero la interrupción más crítica. Este método asegura que las interrupciones más urfentes sean tratadas inmediatamente, mejorando la capacidad de respuesta del sistema.</li>
                        <li> <b>Inhabilitación de interrupciones:</b> Consiste en deshablitar todas las interrupciones mientras se está atendiendo una. Este enfoque simplifica el manejo de interrupciones, pero resulta ineficiente en sistemas donde las interrupciones mas críticas deben ser atendidas sin demora.</li>
                    </ul>
                    <br>
                    
                    <i>Tener interrupciones manejadas por prioridad es más efectivo y logra un sistema mas equilibrado mejorando así el rendimiento del sistema en aplicaciones de tiempo real.</i>
                </p>
                <br>

                <h3>Rutinas de Servicio de Interrupciones (ISR)</h4>
                <p>
                    Las <b>ISR (Interrupt Service Routines)</b> son programas especiales que se ejecutan automáticamente para <i>atender eventos específicos</i> llamados <b>interrupciones</b>. El objetivo principal de cada ISR es <i>gestionar el evento que causó la interrupción y restaurar el sistema</i> para que continúe funcionando normalmente. 
                    <div class="cuadro">
                        <b>Ejemplo Claro de ISR:</b><br>
                        Imagina que estás viendo una película en tu computadora y recibes un mensaje en WhatsApp.<br>
                        La interrupción es el mensaje entrante. El sistema operativo pausa la película momentáneamente, abre WhatsApp para mostrarte la notificación, y luego retoma la película donde se quedó.
                    </div>
                    <br>
                    
                    <ul class="lista">
                        <li>
                            <b>Ubicación de la ISR:</b> Las direcciones de las ISR se encuentran en una estructura conocida como vector de interrupciones que es una tabla en memoria. Cada entrada de la tabla contiene la dirección de inicio de una ISR específica.
                            <div class="cuadro">
                                Si presionas una tecla en tu teclado, la CPU consulta el vector de interrupciones para encontrar la dirección exacta de la ISR que maneja las pulsaciones. Luego, esa ISR registra la tecla que presionaste y permite que sigas usando la computadora sin problemas.
                            </div>
                        </li>
                        <br>
                        
                        <li>
                            <b>Acceso a las ISR:</b> Cuando ocurre la interrupción, el procesador consulta el vector de interrupciones utilizando el número de interrupción para obtener la dirección de la ISR correspondiente. Este número de interrupción actúa como un índice en la tabla del vector. EL procesador luego transfiere el control a la ISR para manejar la interrupción.
                        </li>
                        <br>
                        
                        <i>
                            El diseño eficiente y la correcta ubicación de las ISR son cruciales para el manejo efectivo de las interrupciones. <br> 
                            Al tener una tabla centralizada, el sistema puede acceder de forma rápida a la ISR necesaria, garantizando así la respuesta rápida al evento.   
                        </i>
                    </ul>
                </p>
                <br>
                
                <h3>Beneficios de la Prioridad en Interrupciones y ISR</h3>
                <ul class="lista">
                    <li><b>Mejora de la Respuesta del Sistema:</b> Las interrupciones más urgentes se manejan primero, garantizando que eventos críticos, como alarmas, reciban atención inmediata.</li>
                    <li><b>Optimización de Recursos:</b> La CPU no pierde tiempo en tareas menos importantes si hay eventos prioritarios que atender.</li> <li><b>Mantenimiento de la Integridad del Sistema:</b> Al procesar interrupciones en el orden adecuado, se evita que el sistema falle o se vuelva inestable.</li> <li><b>Flexibilidad en el Manejo de Interrupciones:</b> La prioridad permite adaptar el comportamiento del sistema según las necesidades del momento, garantizando que siempre se atiendan las tareas críticas a tiempo.</li> 
                </ul>
                <br>
                
                <h3>Técnicas para Determinar el Dispositivo que provocó la interrupción</h3>
                <ul class="lista">
                    <li><b>Múltiples lineas de Interrupciones:</b> Cada dispositivo tiene su propia línea de interrupción hacia el procesador, permitiendo identificar rápidamente el origen del evento.</li>
                    <li><b>Consulta de software (polling):</b> La CPU revisa constantemente cada dispositivo para verificar si ocurrió una interrupción, aunque esto consume más tiempo y recursos.</li>
                    <li><b>Consulta de hardware:</b> Se emplea un controlador de interrupciones que consulta automáticamente qué dispositivo generó la señal y notifica al procesador.</li>
                    <li><b>Arbitraje de Bus:</b> Los dispositivos compiten por el control del bus; un mecanismo de prioridad decide cuál tiene permiso para enviar la interrupción primero.</li>
                </ul>
                <br>
                
                <h3>Propiedades de las Interrupciones</h3>
                <ul class="lista">
                    <li><b>Asincronía:</b> Las interrupciones pueden ocurrir en cualquier momento, ya que no dependen del flujo del programa en ejecución.</li>
                    <li><b>Prioridad:</b> Las interrupciones tienen diferentes niveles de prioridad para garantizar que los eventos más críticos se atiendan primero.</li>
                    <li><b>Anidamiento:</b> Una interrupción puede ser interrumpida por otra de mayor prioridad, lo que permite una gestión eficiente de eventos urgentes.</li>
                    <li><b>Restauración del estado:</b> La CPU guarda y recupera el estado del programa interrumpido para continuar la ejecución sin pérdida de datos.</li>
                    <li><b>Eficiencia:</b> Las interrupciones permiten que la CPU no desperdicie tiempo verificando constantemente los dispositivos; sólo actúa cuando es necesario.</li>
                    <li><b>Manejo de errores:</b> Las interrupciones permiten detectar y gestionar errores en operaciones de E/S o hardware, mejorando la robustez del sistema.</li>
                </ul>
                
                <button onclick="scrollToTop('interrupciones')">▲</button>
                <!--<button onclick="goBack()">Volver Atrás</button>-->
            </section>

            <!-- ********************************************************************************************************** MICROCONTROLADORES -->
            <section id="microcontroladores" class="contenido">
                <h2 class="titulo2">MICROCONTROLADORES</h2>
                <h3>Microcontroladores</h3>
                <p>
                    Un <b>microcontrolador</b> es un circuito integrado programable que ejecuta ordenes grabadas en su memoria.  Es un <b>sistema cerrado</b> que incluye CPU, memoria y periféricos en un solo chip. <br>
                    Está diseñado para <b>aplicaciones específicas</b> y control de aplicaciones. Los microcontroladores son comunes en sistemas embebidos, como electrodomésticos, automóviles, dispositivos médicos y sistemas de automatización industrial.
                </p>
                <div class="contenedor-imagen-tabla">
                    <img src="images/adc-microcontrolador.jpg" width="300px" alt="Microcontrolador" title="Microcontrolador">
                    <img src="images/adc-microprocesador.jpeg" width="300px" alt="Microprocesador" title="Microprocesador">
                </div>
                <br>

                <h3>Microprocesadores</h3>
                <p>
                    Un <b>microprocesador</b> es un circuito integrado que <b>contiene la CPU de una computadora</b>. Es un <b>sistema abierto</b> que requiere <b>componentes</b> externos, como memoria y periféricos para <b>poder funcionar</b>. <br>
                    Los microprocesadores se utilizan en una amplia gama de dispositivos, incluido computadoras personales, servidores, dispositivos móviles y sistemas embebidos de alto rendimiento.
                </p>
                <div class="contenedor-imagen-tabla">
                    <table>
                        <tr>
                            <th style="background-color: rgb(31, 34, 34); ;"> </th>
                            <th>MICRO CONTROLADOR</th>
                            <th>MICRO PROCESADOR</th>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Tipo de sistemas</td>
                            <td>Cerrado</td>
                            <td>Abierto</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Buses</td>
                            <td>Inaccesibles desde el exterior</td>
                            <td>Accesibles desde el exterior</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Memoria</td>
                            <td>Interna, incluye RAM, ROM y almacenamiento</td>
                            <td>Externa, necesita componenetes adicionales</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Flexibilidad</td>
                            <td>Limitada a aplicaciones específicas y tareas predefinidas</td>
                            <td>Alta, se puede utilizar una amplia variedad de aplicaciones</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Complejidad del sistema</td>
                            <td>Menor, integrado en un solo chip</td>
                            <td>Mayor, requiere componentes externos como memoria y periféricos</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Costo</td>
                            <td>Generalmente más barato, todo en un solo chip</td>
                            <td>Generlamente más caro, debido a la necesidad de componentes adicionales</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Ejemplos de uso</td>
                            <td>Electrodomésticos, automóviles, sistemas de automatización</td>
                            <td>Computadoras, servidores, dispositivos móviles</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Capacidad de procesamiento</td>
                            <td>Menos potentes, optimizados para eficiencia y tareas específicas</td>
                            <td>Generalemtne más potentes y rápidos</td>
                        </tr>
                    </table>
                </div>
                <br>

                <h2 class="titulo2">ARQUITECTURAS</h2>
                <div class="contenedor-imagen-tabla">
                    <img src="images/adc-arquitecturas.png" width="800px" alt="arquitecturas" title="Arquitecturas">
                </div>
                <br>
                <h3>Arquitectura Harvard</h3>
                <p>
                    La arquitecura Harvard utiliza <b>dos memorias separadas para instrucciones y datos</b>, permitiendo acceso simultáneo a ambas, lo cual puede incrementar el rendimiento. <br>
                    Esta arquitectura es común en <b>microcontroladores</b>, especialmente en aplicaciones donde se requiere un rendimiento elevado y un control preciso, como en sistemas embebidos y dispositivos de tiempo real. <br><br>
                    En este diseño, la CPU está conectada a dos memorias independientes y con buses diferentes: una para las instrucciones y otra para los datos. Esta configuración permite el <b>paralelismo</b>, ya que se puede acceder a los datos e instrucciones de forma independiente y simultánea.<br>
                    Las memorias no necesariamente deben compartir características, por lo que pueden tener distinta longitud de datos e instrucciones, permitiendo una optimización del uso de la memoria en general.<br>
                    El tiempo de acceso a las instrucciones y a los datos puede superponerse, aumetando la velocidad de procesamiento en cada operación.<br><br>
                    Además, facilita la realización de <b>pipeline</b>, una técnica de segmentación que permite al procesador ejecutar una instrucción mientras busca el código de la siguiente.<br> 
                    Las instrucciones y los datos se almacenan en caches separadas para mejorar el rendimiento. <br>
                    Sin embargo, esta arquitectura tiene <u>desventajas</u>. Divide la cantidad de caché entre instrucciones y datos, lo que solo es beneficioso si la frecuencia de lectura de ambos es similar.<br> 
                    También requiere dos buses separados, lo que aumenta la complejidad y el costo de fabricación.
                </p>
                <br>

                <h3>Arquitectura Von Neumann</h3>
                <p>
                    La arquitectura Von Neumann utiliza una sola memoria para almacenar tanto las instrucciones como los datos.<br>
                    Esta arquitectura describe a la computadora con <b>cuatros secciones principales: </b>
                    <ol>
                        <li>Memoria principal</li>
                        <li>Dispositivos de entrada-salida</li>
                        <li>Unidad Centra de Procesamiento (CPU)</li>
                        <li>Las interconexiones o buses entre todos los bloques funcionales del sistema</li>
                    </ol>
                    <br>

                    Es común en <b>microprocesadores</b> y procesadores de señal digital, debido a su simplicidad y flexibilidad, lo que facilita el diseño en sistemas de propósito general, como los que se encuentran en las computadoras personales. <br><br>
                    En este diseño, la CPU está conectada a una <b>memoria principal única</b> (generalmente RAM), donde se guardan tanto las instrucciones del programa como los datos, lo cual hace que el procesamiento sea eficiente debido a que todo está ubicado en un solo lugar. Esto elimina la necesidad de buses separados y permite una búsqueda más directa y eficiente. <br>
                    Sin embargo, <b>requiere un mecanismo para distinguri entre datos e instrucciones, ya que se almacenan en el mismo formato.</b><br><br>
                    La arquitectura de Von Neumann es relativamente fácil de diseñar y eficiente en el uso de la memoria, ya que no desperdicia espacio de almacenamiento al ser usada por ambos tipos de información. Una particularidad de esta arquitectura es que la <b>conexión se realiza mediante un único bus.</b> El tamaño de las instrucciones es fijado por el ancho del bus; por ejemplo, un microprocesador de 8 bits con un bus de 8 bits tendrá que manejar datos e instrucciones en unidades de 8 bits (bytes). Si se necesita acceder a una instrucción que excede el ancho de bus, se deberán realizar múltiples accesos a la memoria.<br><br>
                    Tener un único bús hace más lenta la respuesta del microprocesador, ya que no puede buscar en memoria una nueva instrucción mientras está transfiriendo datos de la instrucción anterior.<br>
                    Ademas, <b>requiere de dos ciclos de reloj para leer la memoria, uno para las instrucciones y otro para los datos.</b><br>
                    El cuello de botella de Von Neumann se puede mejorar de varias maneras, como con el uso de memoria caché entre la CPU y la memoria princial, o con vías independientes para instrucciones y datos (arquitectura Harvard modificada), para reducir el acceso a memoria.
                </p>
                <br>

                <h2 class="titulo2">INSTRUCCIONES (CISC - RISC)</h2>
                <h3>Conjunto de instrucciones</h3>
                <p>
                    Los <b>microprocesadores</b> suelen ser <b>CISC (Complex Instruction Set Computing)</b>, estos tienen un conjunto de instrucciones grande y complejo, diseñado para realizar tareas complejas en pocas líneas de código. (Secuenciadores microprogramados)
                    <ul class="lista">
                        <li>
                            <b>Características</b>
                            <ul class="lista" style="list-style: circle;">
                                <li><b>Conjunto de instrucciones grandes:</b> Hay muchas instrucciones disponibles, lo que permite realizar tareas complejas en una sola línea de código.</li>
                                <li><b>Complejidad:</b> Hay muchas instrucciones disponibles, lo que permite realizar tareas complejas en una sola línea de código.</li>
                                <li><b>Eficiencia en codificación:</b> Los programadores pueden escribir programas más cortos y complejos, lo que puede reducir el tiempo de desarrollo.</li>
                                <li><b>Variedad de modos de direccionamiento:</b> Permite diferentes formas de acceder a la memoria, lo que añade flexibilidad.</li>
                                <li><b>Mayor consumo de energia:</b> Por lo general, CISC consume más energía debido a su complejidad.</li>
                            </ul>
                        </li>
                        <li>
                            <b>Aplicaciones:</b> Utilizados en computadoras personales, servidores y estaciones de trabajo donde la flexibilidad y la capacidad de realizar tareas complejas son cruciales.
                        </li>
                    </ul>
                    <br>

                    Los <b>microcontroladores</b> suelen ser <b>RISC (Reduced Instruction Set Computing)</b>, estos tienen un conjunto de instrucciones más pequeño y simplificado, optimizado para realizar operaciones básicas de manera rápida y eficiente. Esto es beneficioso para aplciaciones embebidas que requieren alta eficiencia y bajo consumo de energía.
                    <ul class="lista">
                        <li>
                            <b>Características</b>
                            <ul class="lista" style="list-style: circle;">
                                <li><b>Conjunto de instrucciones pequeño:</b> Hay menos instrucciones, lo que simplifica el diseño del procesador.</li>
                                <li><b>Simplicidad:</b> La arquitectura es más sencilla, lo que facilita su implementación.</li>
                                <li><b>Optimización para operaciones básicas:</b> Está diseñado para ejecutar instrucciones simples rápidamente.</li>
                                <li><b>Eficiencia en la ejecución:</b> Las instrucciones se ejecutan de manera más eficiente, gracias a la simplicidad del conjunto.</li>
                                <li><b>Pipeline:</b> Permite que diferentes etapas de ejecución de instrucciones se realicen simultáneamente, aumentando la eficiencia.</li>
                            </ul>
                        </li>

                        <li>
                            <b>Aplicaciones:</b> Ideal para aplicaciones embebidas que requieren alta eficiencia y bajo consumode energia, como electrodomésticos, automóviles, dispositivos médicos y sistemas de automatización industrial.
                        </li>                        
                    </ul>
                    <br>

                    Cabe destacar que el análisis de instrucción no es necesario en <b>RISC</b> porque no hay microprograma, RISC trabaja con <b>secuenciadores cableados.</b><br>
                    En cuanto al cálculo de la dirección del operando, las <b>transferencia se hacen entre registros</b>, y la búsqueda del operando no es necesaria ya que el <b>operando está en registros de acceso</b> muy rápido.<br>
                    Además, cada instruccion en RISC toma solo <b>un solo ciclo de reloj</b>, lo que constribuye a su alta eficiencia.
                    <br>

                    <div class="contenedor-imagen-tabla">
                        <table>
                            <tr>
                                <th> Características</th>
                                <th>RISC</th>
                                <th>CISC</th>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Simplicidad de instrucciones</b></td>
                                <td>Instrucciones simples, típicamente <b>1 ciclo de ejecución</b></td>
                                <td>Instrucciones complejas, pueden tomar <b>múltiples ciclos de ejecución</b></td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Acceso a Memoria</b></td>
                                <td>Referencias a memoria solo con instrucciones LOAD/STORE</td>
                                <td>Cualquier instrucción puede referenciar a la memoria.</td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Pipeline</b></td>
                                <td>Más eficaz, con menos etapas</td>
                                <td>Menos eficaz, con más etapas</td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Ejecución de instrucciones</b></td>
                                <td>Ejecutadas directamente por el <b>hardware</b></td>
                                <td>INterpretadas por un <b>microprograma</b></td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Formato de instrucciones</b></td>
                                <td>Formato <b>fijo</b> de instrucciones</td>
                                <td>Formato <b>variable</b> de instrucciones</td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Cantidad de ejecuciones</b></td>
                                <td>Poca instruccioens y modos de instrucción</td>
                                <td>Muchas instrucciones y modos</td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Complejidad del Sistema</b></td>
                                <td>La complejidad se encuentra en el <b>compilador</b></td>
                                <td>Complejidad en el <b>microprograma</b></td>
                            </tr>
                            <tr>
                                <td class="fila-descripcion"><b>Registro</b></td>
                                <td>Conjunto <b>múltiple</b> de registros</td>
                                <td>Conjunto <b>único</b> de registros</td>
                            </tr>
                        </table>
                    </div>
                    <br>

                    En conclusión, se puede decir que las máquinas RISC completan todas sus instrucciones en un solo ciclo de reloj, es por eso que es más rápida que las máquinas CISC, y en RISC contamos con un gran manejo de registros. Sin embargo, para efectuar una operación de mayor complejidad (como multipicaciones o divisiones) necesitamos más instrucciones como en CISC.<br>
                    Por eso un usuario debe definirse a favor o en contra de una arquitectura en función de la aplicación que desea realizar.
                </p>
                <br>

                <h2 class="titulo2"> PIPELINE EN MICROCONTROLADORES</h2> 
                <p>
                    El <b>pipeline</b> en los <b>microcontroladores es una técnica segmentada</b> utilizada para aumentar la eficiencia y la velocidad de procesamiento al dividir la <b>ejecución de una tarea en múltiples etapas independientes.</b><br>
                    Estas etapas pueden ejecutarse simultáneamente en diferentes partes del hardware del microcontrolador. 
                    <br>
                    
                    La tarea de ejecutar una instrucción se divide en varias etapas como:
                    <ul class="lista">
                        <li>la búsqueda de la instrucción (fetch), </li>
                        <li>la decodifcación (decode), </li>
                        <li>la ejecución (execute), </li>
                        <li>la memoria (memory), y </li>
                        <li>la escritura del resultado (write-back)</li>
                    </ul> 
                    <br>

                    Cada una de estas etapas puede trabajar en diferentes instruciones al mismo tiempo.<br>
                    <div class="cuadro">
                        Por ejemplo, mientras una instrucción está siendo decodificada, otra puede estar siendo buscada y otra puede estar siendo ejecutada. Sin embargo, las <u>dependencias</u> de datos y otros problemas pueden causar ralentizaciones, por lo que se requieren diversas estrategias para mitigar estos problemas y mantener el flujo de instrucciones lo más fluido posible.
                    </div>
                    <br><br>

                    <h3>Estrategias para mitigar problemas en el pipeline</h3>
                    <h4>Incluyen:</h4>
                    <ul class="lista">
                        <li>Interbloque (Stalling)</li>
                        <li>Reordenamiento de instrucciones</li>
                        <li>Bypassing / Forwarding</li>
                        <li>Predicción de saltos</li>
                    </ul>
                    <br>

                    <h4>Ventajas</h4>
                    <ul class="lista">
                        <li>Mayor rendimiento</li>
                        <li>Eficiencia</li>
                    </ul>
                    <br>

                    <h4>Desventajas</h4>
                    <ul class="lista">
                        <li>Interbloqueos</li>
                        <li>Complejidad de Control</li>
                    </ul>
                </p>
                <br>

                <h3>Comparación con Microprocesadores</h3>
                <p>
                    Los <b>microprocesadores</b>, diseñados para tareas más complejas y de alto rendimiento, implementan pipeline de manera más agresiva y sofisticada.<br>
                    Tienen más recursos disponibles y pueden permitirse un pipeline más profundo y complejo, lo cual mejora significativamente el rendimiento costa de mayor consumo de energía.<br><br>
                    Por otro lado, los <b>microcontroladores</b> están diseñados para aplicaciones específicas y de menor consumo energético, por lo que su implementación de pipeline tiende a ser más sencilla y menos profunda.
                </p>
                <br>

                <h3>Comparación con Procesador sin Pipeline</h3>
                <p>
                    <ul>
                        <li><b>Sin Pipeline: </b> Cada instrucción debe completarse antes de que la siguiente comience, lo que puede resultar en una ejecución más lenta.</li>
                        <li><b>Ventaja de Sin Pipeline: </b> Diseño más sencillo y menor complejidad en la unidad de control.</li>
                    </ul>
                </p>
                <br><br>

                <h2 class="titulo2">FORMATO DE INSTRUCCIONES ORTOGONAL</h2>
                <p>
                    El <b>formato de instrucciones ortogonal</b> es un diseño de conjunto de instrucciones en el que <b>cualquier operación puede ser realizada con cualquier registro o modo de direccionamiento disponible.</b> 
                    <br>Esto proporciona una gran flexibilidad y simplicidad en la programación, ya que permite utilizar <b>cualquier combinación de registros y modos de direccionamiento para cualquier instrucción.</b><br>
                    Sin embargo, este formato puede resultar en instrucciones de mayor tamaño, lo que aumenta la complejidad de decodificación, y también incrementa la complejidad del hardware al requerir soporte  para todas las combinaciones posibles.
                    <br><br>

                    <h4>Ventajas</h4>
                    <ul class="lista">
                        <li>Flexibilidad: Facilita la programación al permitir una mayor flexibilidad en el uso de registros y modos de direccionamiento.</li>
                        <li>Simplicidad: Simplifica el diseño del compilador y el código ensamblador</li>
                    </ul>
                    <br><br>

                    <h4>Desventajas</h4>
                    <ul class="lista">
                        <li>Tamaño de instrucciones: Puede resultar en isntrucciones de mayor tamaño, lo que aumenta la complejidad de decodificación.</li>
                        <li>Costo: Aumenta la complejidad de hardware, ya que se necesita soporte para todas las combinaciones posibles de registros y modos de direccionamiento.</li>
                    </ul>
                </p>
                <br><br>

                <h3>Amplio banco de registros:</h3>
                <p>
                    Un <b>amplio banco de registros</b> se refiere a disponer de una <b>gran cantidad de registros</b> accesibles por el procesador de operaciones rápidas. ESto reduce la necesidad de acceder a la memoria princial, acelerando el procesamiento y permitiendo que más variables temporales y datos intermedio se almacenan en registros, optimizando la eficiencia del código.<br>
                    Sin embargo, tener más registros puede complicar la gestión de contexto durante interrupciones o cambios de tarea, y aumentar el costo y la complejidad del hardware en comparación con un <b>banco de registro limitado</b>, que requiere más acceso a memoria y puede ralentizar el procesamiento, aunque <b>simplifica el diseño y reduce los costos</b>
                    <br><br>

                    <h4>Ventajas</h4>
                    <ul class="lista">
                        <li>Eficiencia: Reduce la necesidad de acceder a la memoria principal, lo que acelera el procesamiento.</li>
                        <li>Optimización de Código: Permite que más variables temporales y datos intermedios se almacenan en registros, mejorando la eficiencia del código.</li>
                    </ul>
                    <br><br>

                    <h4>Desventajas</h4>
                    <ul class="lista">
                        <li>Complejidad de manejo: Un mayor número de registros puede complicar la gestión de contexto durante interrupciones o cambios de tarea.</li>
                        <li>Costo de hardware: Más registros pueden aumentar el costo y la complejidad del hardware</li>
                    </ul>
                    <br><br>

                    <h4>Comparación de Regitro Limitado</h4>
                    <ul class="lista">
                        <li>Registros limitados: Menos registros requieren más accesos a memoria, lo que puede ralentizar el procesamiento.</li>
                        <li>Ventaja de registros limitados: Diseño más sencillo y menor costo de hardware.</li>
                    </ul>
                </p>

                <button onclick="scrollToTop('microcontroladores')">▲</button>
                <!--<button onclick="goBack()">Volver Atrás</button>-->
            </section>

            <!-- ********************************************************************************************************** PIPELINE -->
            <section id="pipeline" class="contenido">
                <h2 class="titulo2">PIPELINE</h2>
                <p>
                    Pipeline es una técnica utilizada en los procesadores para mejorar su rendimiento, permitiendo que varias instrucciones se ejecuten al mismo tiempo pero en diferentes <b>etapas</b> del proceso.</b><br>
                    La ejecución de <b>una instrucción se divide en varias etapas, y múltiples instrucciones pueden estar en diferentes etapas simultáneamente</b>, lo que permite que el procesador trabaje en varias instrucciones al mismo tiempo.
                </p>
                <br>

                <h3>Beneficios</h3>
                <ul class="lista">
                    <li><b>Aumento del rendimiento:</b> Incrementa el número de instruccioens que se pueden ejecutar por unidad de tiempo.</li>
                    <li><b>Paralelismo implícito:</b> Cada etapa del pipeline traba en una instrucción diferente simultáneamente.</li>
                    <li><b>Aprovechameinto de recursos:</b> Permite un uso más eficiente de los recursos del procesador.</li>
                    <li><b>Reducción del tiempo de ejecución promedio:</b> Reduce el tiempo promedio por instrucción en un flujo continuo de instrucciones.</li>
                    <li><b>Frecuencia de reloj mas alta:</b> Al dividir el trabajo en etapas más pequeñas, cada etapa puede completarse más rápidamente, permitiendo una frecuencia de reloj potencialmente más alta.</li>
                </ul>
                <br>

                <h3>Aplicaciones</h3>
                <ul class="lista">
                    <li><b>Procesadores de Alto Rendimiento:</b> Utilizado ampliamente en procesadores modernos para mejorar el rendimiento en aplicaciones de alta demanda, como gráficos, juegos, procesamiento de datos y servidores</li>
                    <li><b>Microcontroladores Avanzados:</b> Algunos microcontroladores también incorporan pipelines para mejorar su eficiencia en apilcaciones embebidas críticas.</li>
                </ul>
                <br>

                <h3>Instrucciones que generan problemas:</h3>
                <p>
                    Las instrucciones de salto (tanto condicionales como incondicionales) son aquellas que alteran el flujo normal de ejecución de un programa.<br>
                    Por ejemplo, si tienes una instrucción que dice <i>"si la condición se cumple, ve a la línea X"</i>, esto puede causar que el procesador se detenga y cambie de dirección, lo que interrumpe el flujo del pipeline.<br>
                    Si se efectúa un salto, las etapas que se hayan ejecutado de las instrucciones siguientes del pipeline <b>no sirven y se deben limpiar todos los registros.</b>
                </p>
                <br>

                <h3>Soluciones</h3>
                <ul class="lista">
                    <li>
                        <b>Dejar de cargar el pipeline hasta atender el salto (Detener el Pipeline):</b><br>
                        - <b>Descripción:</b> Cuando se detecta un salto, el pipeline se detiene y espera a que se procese el salto antes de continuar.<br>
                        - <b>Desventaja:</b> Este método es ineficiente porque hay un período de tiempo en el que no se están ejecutando instrucciones, lo que disminuye el rendimiento.
                            <!--Esto no es muy efectivo debido a que la espera tarda mucho tiempo y no se ahce nada durante la misma.-->
                    </li>
                    <br>

                    <li>
                        <b>Tratar el salto como una instrucción Normal (y seguir cargando el pipeline).</b><br>
                        - <b>Descripción:</b> Continuar cargando el pipeline incluso si hay un salto. Si se ejecuta el salto, se limpia el pipeline y se empieza a cargar desde la nueva dirección.<br>
                        - <b>Desventaja:</b> Similar al primero, limpiar el pipeline también toma tiempo y recursos, ya que debes restaurar los registros a su estado anterior.
                        
                        <!--
                            Si el salto es existoso, se limpia el pipeline y se cargar a partir de la posición a la que lleva el salto.<br>
                            Sin embargo, tampoco es muy efectiva, debido a que la limpieza del pipeline demanda mucho tiempo. Limpiar consiste en volver a cargar el valor de los registros que las instrucciones hayan modificado; para esto se necesita guardar los valores previos.
                        -->
                    </li>
                    <br>

                    <li>
                        <b>Predicción de saltos:</b><br>
                        - <b>Descripción:</b> Intentar anticipar si un salto se realizará o no. Esto se puede hacer de dos maneras:
                        <div class="contenedor-imagen-tabla">
                            <u>Estática:</u> El compilador predice el comportamiento de los saltos basándose en el análisis del código. Por ejemplo, puede asumir que un salto condicional a menudo se cumple.<br><br> 
                            <u>Dinámica:</u> Se registran las decisiones de salto en tiempo de ejecución. Se crea una tabla que guarda el historial de saltos anteriores, utilizando un contador (o bits) para ayudar a predecir si el salto ocurrirá. Si el salto se efectúa, se incrementa el contador; si no, se decrementa.<br>
                        </div><br>
                        - <b>Ventaja:</b> Esta técnica puede reducir el número de veces que se necesita limpiar el pipeline, mejorando el rendimiento en general.
                        </li>
                        <!--
                            Consiste en predecir los saltos anotando el comportamiento del programa en saltos anteriores. Puede ser estática o dinámica.
                            <ul>
                                <li><b>Estática:</b> El compilador hace una suposición por cad auan de las isntrucciones de salto que genera.</li>
                                <li><b>Dinámica:</b> En tiempo de ejecución, el microprograma constituye una tabla con las direcciones que contiene saltos y guarda un registro de comportamientos de cada uno (si se efectuó o no). Se asocian dos o más bits a cada isntrucción de salto, que actúan como un contador: si el salto se efectúa se suma, si no se efectuó se resta.</li>
                            </ul></li>
                        -->
                </ul>
                <br>

                <h3>PIPELINE genérico de cinco etapas de una máquina-CISC</h3>
                <p>En un procesador clásico, la ejecución de una instrucción se divide en varias etapas, como:</p>
                <ol>
                    <li><b>Fetch (Búsqueda):</b> Obtener la instrucción de la memoria. Se obtiene la instrucción de la memoria de instrucciones y se carga en el registro de instrucciones.</li>
                    <li><b>Decode (Decodificación):</b> Decodificar la instrucción y lees los registros fuente. La instrucción se decodifica (interpreta) para determinar qué operación se va a realizar, y se leen los registros fuentes necesarios.</li>
                    <li><b>Execute (Ejecución):</b> Realizar la operación aritmética o lógica (ALU). Se realiza la operación aritmética o lógica especificada por la instrucción.</li>
                    <li><b>Memory Acces (Memoria):</b> Acceder a la memoria para leer o escribir datos. <b>Si la instrucción es una opearción de carga o almacenamiento,</b> se accede a la memoria para leer o escribir datos.</li>
                    <li><b>Write Back (Escritura):</b> Escribir el resultado en los registros. <b>El resultado de la operación se escribe</b> en el registro de destino.</li>
                </ol>
                <br>
                <!--
                    Fetch (Búsqueda): Se lee la instrucción de la memoria.
                    Decode (Decodificación): Se interpreta qué hace la instrucción.
                    Execute (Ejecución): Se realiza la operación (por ejemplo, una suma).
                    Memory (Memoria): Se leen o escriben datos en la memoria (si es necesario).
                    Write-back (Escritura): Se guarda el resultado en un registro.
                    Sin pipeline, cada instrucción se ejecutaría por completo antes de pasar a la siguiente, como si una persona tuviera que terminar un producto antes de comenzar otro.
                -->
                <div class="cuadro">
                    <p>
                        <h4><u>Ejemplo en acción</u></h4>
                        <br>
                        Supongamos que tenemos las siguientes instrucciones en secuencia:
                        <ul class="lista">
                            <li>CARGAR A: Carga un valor en el registro A.</li>
                            <li>CARGAR B: Carga un valor en el registro B.</li>
                            <li>SUMAR A, B: Suma los valores de A y B.</li>
                            <li>ALMACENAR C: Almacena el resultado en la dirección de memoria C.</li>
                        </ul>
                        <br>

                        <h4><u>Funcionamiento del Pipeline:</u></h4>
                        <ul class="lista">
                            <li>Ciclo 1: En el ciclo de reloj 1, se obtiene (fetch) la instrucción "CARGAR A".</li>
                            <li>Ciclo 2: En el ciclo 2, se decodifica (decode) "CARGAR A" y se obtiene la instrucción "CARGAR B".</li>
                            <li>Ciclo 3: En el ciclo 3, se ejecuta (execute) "CARGAR A" y se obtiene "SUMAR A, B".</li>
                            <li>Ciclo 4: En el ciclo 4, se accede a la memoria (memory access) para "CARGAR B" y se ejecuta "SUMAR A, B".</li>
                            <li>Ciclo 5: En el ciclo 5, se escribe de vuelta (write back) el resultado de "SUMAR A, B" y se accede a la memoria para "ALMACENAR C".</li>
                        </ul>
                    </p>
                </div>
            <p>
                Cada etapa de pipeline puede procesar una instrucción diferente simultáneamente. Esto significa que mientras una instrucción está en la etapa de decodificación, otra puede estar en la etapa de búsqueda y otra en la etapa de ejecución. De esta manera, múltiples instrucciones pueden estar en diferentes etapas del pipeline al mismo tiempo, permitiendo que el procesador trabaje en varias instrucciones simultáneamente.
            </p>
            <br>

            <h3>Diagrama de tiempo indicando la ejecución de instrucciones en ese PIPELINE de 5 etapas:</h3>
                <img src="images/adc-pipeline.png" alt="pipeline" style="width: 600px;">
                <div class="contenedor-imagen-tabla">
                    <table>
                        <tr>
                            <th> CICLO</th>
                            <th>1</th>
                            <th>2</th>
                            <th>3</th>
                            <th>4</th>
                            <th>5</th>
                            <th>6</th>
                            <th>7</th>
                            <th>8</th>
                            <th>9</th>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Fetch</td>
                            <td>A</td>
                            <td>B</td>
                            <td>C</td>
                            <td>D</td>
                            <td>E</td>
                            <td>F</td>
                            <td>G</td>
                            <td>H</td>
                            <td>I</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Decode</td>
                            <td>-</td>
                            <td>A</td>
                            <td>B</td>
                            <td>C</td>
                            <td>D</td>
                            <td>E</td>
                            <td>F</td>
                            <td>G</td>
                            <td>H</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Execute</td>
                            <td>-</td>
                            <td>-</td>
                            <td>A</td>
                            <td>B</td>
                            <td>C</td>
                            <td>D</td>
                            <td>E</td>
                            <td>F</td>
                            <td>G</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Memory Access</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>A</td>
                            <td>B</td>
                            <td>C</td>
                            <td>D</td>
                            <td>E</td>
                            <td>F</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">Write Back</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>A</td>
                            <td>B</td>
                            <td>C</td>
                            <td>D</td>
                            <td>E</td>
                        </tr>
                    </table>
                </div>
                <br>

                <h4>Explicación:</h4>
                    <ol>
                        <li><b>Ciclo 1:</b> Se inicia el <b>Fetch</b> de la instrucción <b style="color: brown;">A</b></li>
                        <li><b>Ciclo 2:</b> <b style="color: brown;"> A</b> pasa a <b>Decode</b>, mientras <b style="color: rgb(43, 82, 43);">B</b> inicia su <b>Fetch</b></li>
                        <li><b>Ciclo 3:</b> <b style="color: brown;"> A</b> pasa a <b>Execute</b>, <b style="color: rgb(43, 82, 43);">B</b> a <b>Decode</b>, <b style="color: rgb(100, 100, 185);"">C</b> inicia su <b>Fetch</b></li>
                        <li><b>Ciclo 4:</b> <b style="color: brown;"> A</b> pasa a <b>Memory Access</b>, <b style="color: rgb(43, 82, 43);">B</b> a <b>Execute</b>, <b style="color: rgb(100, 100, 185);"">C</b> a <b>Decode</b>,<b style="color: rgb(185, 176, 100);""> D</b> inicia su <b>Fetch</b></li>
                        <li><b>Ciclo 5:</b> <b style="color: brown;"> A</b> pasa a <b>Write Back</b>, <b style="color: rgb(43, 82, 43);">B</b> a <b>Memory Access</b>, <b style="color: rgb(100, 100, 185);"">C</b> a <b>Execute</b>,<b style="color: rgb(185, 176, 100);""> D</b> a <b>Decode</b>, <b style="color: rgb(100, 179, 185);""> E</b> inicia su <b>Fetch</b></li>
                    </ol>
                    A partir del ciclo 5, el pipeline está completamente lleno y se están procesanto 5 instrucciones simultánemaente, una en cada etapa.
                    <br><br>

                    Este diagrma muestra claramente:
                    <ol>
                        <li>Las 5 etapas de pipeline CISC (Fetch, Decode, Execute, Memory Access, Write Back)</li>
                        <li>Cómo múltiples instrucciones (A, B, C, etc.) avanzan por las etapas del pipeline.</li>
                        <li>El paralelismo alcanzado, con una nueva instrucción iniciando cada ciclo.</li>
                        <li>Que a partir del ciclo 5, se completa una instrucción por ciclo (aunque cada instrucción tarda 5 ciclos en completarse).</li>
                    </ol>
                    <br>

                <h4>Cuadro con ejemplo dado previamente:</h4>
                <div class="contenedor-imagen-tabla">
                    <table>
                        <tr>
                            <th>CICLO DE RELOJ</th>
                            <th>CARGAR A</th>
                            <th>CARGAR B</th>
                            <th>SUMAR A,B</th>
                            <th>ALMACENAR C</th>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">1</td>
                            <td>Fetch</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">2</td>
                            <td>Decode</td>
                            <td>Fetch</td>
                            <td>-</td>
                            <td>-</td>

                        </tr>
                        <tr>
                            <td class="fila-descripcion">3</td>
                            <td>Execute</td>
                            <td>Decode</td>
                            <td>Fetch</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">4</td>
                            <td>Memory Access</td>
                            <td>Execute</td>
                            <td>Decode</td>
                            <td>Fetch</td>
                        </tr>
                        <tr>
                            <td class="fila-descripcion">5</td>
                            <td>Write Back</td>
                            <td>Memory Acces</td>
                            <td>Execute</td>
                            <td>Decode</td>
                        </tr>
                        <tr></tr>
                            <td class="fila-descripcion">5</td>
                            <td>-</td>
                            <td>Write Back</td>
                            <td>Memory Acces</td>
                            <td>Execute</td>
                        </tr>
                    </table>
                </div>
                <br>

                <h4>Ciclos de reloj:</h4>
                <ul class="lista">
                    <li>Ciclo 1: CARGAR A (Fetch): La instrucción CARGAR A se está obteniendo de la memoria. En este ciclo, el procesador está buscando la instrucción y cargándola en el registro de instrucciones..</li>
                    <li>Ciclo 2: "CARGAR A" se decodifica (Decode), mientras que "CARGAR B" se obtiene (Fetch).</li>
                    <li>Ciclo 3: "CARGAR B" se decodifica (Decode) y "SUMAR A, B" se obtiene (Fetch).</li>
                    <li>Ciclo 4: "SUMAR A, B" se decodifica (Decode) y "CARGAR A" se ejecuta (Execute). También, "ALMACENAR C" se obtiene (Fetch).</li>
                    <li>Ciclo 5: "ALMACENAR C" se decodifica (Decode), "SUMAR A, B" se ejecuta (Execute) y "CARGAR A" realiza la operación de acceso a memoria (Memory Access).</li>
                    <li>Ciclo 6: "CARGAR A" escribe de vuelta el resultado (Write Back).</li>
                </ul>
                <br>
                
                <h3>Pipeline en RISC</h3>
                <p>
                    El pipeline en arquitecturas RISC (Reduced Instruction Set Computing) se caracteriza por su simplicidad y eficiencia, consta de solos dos etapas principales:
                    <ol>
                        <li><b>Búsqueda del operando:</b> En esta fase, el procesador localiza y obtiene los datos necesarios para ejecutar la instrucción. Estos datos suelen estar en <b>registros</b>, lo que permite un acceso rápido.</li>
                        <li><b>Ejecución de la instrucción:</b> Una vez obtenidos los operandos, el procesador realiza la operación especificada por la instrucción.</li>
                    </ol><br>

                    A diferencia de las arquitecturas CISC (Complex Instruction Set Computing), RISC, no necesariamente utiliza una etapa de análisis de instrucciones compleja.<br>
                    Esto se debe a que, en muchos casos, <b>RISC no emplea micropogramación</b>, sino que puede traba con secuenciadores cableados, lo que significa que las instrucciones están directamente implementadas en hardware.<br>
                    Las operaciones en RISC se realizan principalmente entre registros, lo que incluye el cálculo de direcciones de operandos y las transferencias de datos. Este enfoque de registros a registro contribuye a la velocidad y eficiencia del pipeline.<br>
                    La arquitectura RISC se beneficia aún más cuando se implementa en una arquitectura Harvard. Esta combinación permite la separación física de las memorias de instruccioens y datos, posiblitando el acceso simultáneo a ambas y aumentando así el rendimiento del pipeline.
                    <br><br>

                    Aunque optimizada, la segmentación en RISC, sigue presentando ciertos problemas. Anteriorimente, vimos que las instrucciones de salto y las lecturas a memoria generan complicaciones al segmentar instrucciones.<br>
                    Para soluciones estos problemas, el compilador o ensamblador puede insertar una operación NOOP en el flujo de instrucciones. La instrucción <b>NOOP, irónicamente, no realiza ninguna operación sobre los datos. En cambio, resulta útil para generar un retardo en el flujo de instrucciones.</b> Esto es necesario cuando, en una operación segmentada, se necesita evaluar una condición de salto o cuando una instrucción depende de un dato que está a punto de ser actualizado, y usualmente se coloca después de que todos los registros involucrados han sido cargados o escritos.
                    <br><br>

                    Existe una forma de optimizar el programa mediante estos retardos, especificamente utilizando lo que se conoce como salto retardado y salto retardado optimizado.
                    <ul class="lista">
                        <li><b>Salto Retardado:</b> Consiste en colocar un NOOP después del salto. Esto tiene la ventaja de que, para cuando el flujo del programa alcanza el NOOP, la instrucción de salto ya se está ejecutando y el salto ya se habrá realizado. Además, el programa no tiene que esperar hasta completar una operación de lectura/escritura realizada previamente para saltar, <b>garantizando que nose comience a buscar la operación siguiente al salto antes de tiempo.</b></li>
                        <li><b>Salto con Retardo Optimizado:</b> Hace algo similar, pero reemplaza el NOOP por la instrucción previa al salto, intercambiando sus lugares. Así, primero se evalúa y ejecuta el salto. Dado que la instrucción anterior era necesaria y al mismo tiempo es la próxima a ejecutarse, se garantiza que el salto se realice correctamente y que la instrucción con la que se cambío de lugar ya esté en proceso de búsqueda para su ejecución.</li>
                    </ul><br>

                    <b>Cabe destacar que este último método sólo se aplica a saltos incondicionales, llamdas y retornos.</b> Para saltos condicionales, el compilador utiliza un salto con retardo normal, insterando el NOOP después del salto. De esta forma, al evaluar la condición del salto, no se realiza ninguna otra operación que comprometa el estado del procesador y de los registros.
                </p>
                <button onclick="scrollToTop('pipeline')">▲</button>
                <!--<button onclick="goBack()">Volver Atrás</button>-->
            </section>

            <!-- ********************************************************************************************************** ENTRADA - SALIDA -->
            <section id="entrada-salida" class="contenido">
                <h2 class="titulo2">ENTRADA - SALIDA</h2>
                <p>
                    Además del procesador y los módulos de memoria, las computadoras tienen otros componentes importantes llamados <b>módulos de entrada/salida.</b> Estos módulos se encargan de controlar los dispositivos externos (como el teclado, la impresora, o unidad USB) y permitir que se comuniquen con el resto del sistema a través del <b>bus del sistema.</b> El bus del sistema es una especie de <b>autopista de datos</b> que conecta todos los componentes clave de la computadora, permitiendo el intercambio de información entre el procesador, la memoria y los dispositivos de entrada/salida.
                    <br><br>
                    Imagina que el módulo de entrada/salida es como un <b>puente</b> que conecta un dispositivo externo con la computadora. No es solo un cable que conecta un periférico (como un teclado o una impresora) al sistema, sino que también tiene cierta "inteligencia", lo que significa que tiene la <b>capacidad de tomar decisiones</b> y gestionar el flujo de datos entre el dispositivo y la computadora.
                    <br><br>
                    En lugar de que la computadora tenga que hacer todo el trabajo cada vez que envía o recibe datos de un periférico, el módulo de entrada/salida puede <b>organizar</b> la comunicación y hacerla más eficiente. Así, se asegura de que los datos vayan y vengan de manera ordenada, sin que el procesador (la parte principal que hace los cálculos) se tenga que preocupar por los detalles de cómo se comunican los periféricos.
                </p>
                
                <h3>Módulos de entrada/salida</h3>
                <p>
                    Las principales funciones y requisitos de un módulo de entrada/salida se agrupan en las siguientes categorías:
                    <ul class="lista">
                        <li style="color: rgb(77, 165, 165);"><b>Control y temporización(1)</b></li>
                        <li style="color: rgb(86, 182, 86);"><b>Comunicación con el procesador(2)</b></li>
                        <li style="color: rgb(216, 216, 79);"><b>Comunicación con los dispositivos(3)</b></li>
                        <li style="color: rgb(214, 164, 71);"><b>Almacenamiento temporal de datos(4)</b></li>
                        <li style="color: rgb(88, 89, 190);"><b>Detección de errores(5)</b></li>
                    </ul>
                    <br>

                    <!--1- Control y temporización-->
                    <h4 style="color: rgb(77, 165, 165);">1- Control y temporización</h4>
                    En cualquier momento, el procesador puede comunicarse con uno o más dispositivos externos, dependiendo de las necesidades de entrada/salida del programa. Los recursos internos, como la memoria principal y el bus del sistema, deben compartirse entre diversas actividades, incluida la entrada y salida de datos. Por lo tanto, la función de entrada/salida requiere mecanismos de <b style="color: rgb(77, 165, 165);">control y temporización</b> para coordinar el tráfico de datos entre los recursos internos y los dispositivos externos.
                    <br>
                    <b>Los módulos de E/S gestionan este tráfico de datos entre el procesador, la memoria y los dispositivos externos (como impresoras o discos duros) para asegurar un funcionamiento eficiente y evitar conflictos.</b>
                    
                    <div class="contenedor-imagen-tabla">
                        <h4>Ejemplo:</h4> Imaginá que estás en un cruce de calles. El semáforo controla quién pasa primero, ya sea los autos o los peatones. De la misma forma, el módulo de E/S asegura que los datos se muevan de manera ordenada entre la computadora y los dispositivos, sin causar "choques" de información.<br>
                        En este caso, si se tiene un teclado y una impresora conectados, el módulo de E/S decide cuándo enviar los datos que escribes y cuándo recibir información de la impresora, sin que haya interferencias.
                    </div>
                    <br>

                    <h3>ESCRITURA DE UN MÓDULO DE ENTRADA/SALIDA</h3>
                    <!-- 2- Comunicación con el procesador -->
                    <b style="color: rgb(86, 182, 86);">2- Comunicación con el procesador</b><br>
                    La lógica en un módulo de entrada/salida interactúa con el procesador a través de una serie de líneas de control. Estas líneas son utilizadas por el procesador para proporcionar órdenes al módulo de E/S. Algunas de las líneas de control también pueden ser empleadas por el propio módulo.<br><br>
                    El módulo de E/S debe recibir órdenes del procesador (como "escribe este dato en la impresora") y reportar el estado de los dispositivos (por ejemplo, "la impresora está lista"). Cada módulo de entrada/salida tiene una dirección única o, si controla más de un dispositivo externo, un conjunto único de direcciones. Por último, el módulo de E/S posee la lógica específica para la interfaz con cada uno de los dispositivos que controla.
                    
                    <div class="contenedor-imagen-tabla">
                        <h4>Ejemplo:</h4> Supongamos que el procesador es como un jefe de cocina y el módulo de E/S es un mesero. El jefe (procesador) le dice al mesero (módulo de E/S) qué hacer, como "lleva esta comida a la mesa 3", y el mesero reporta si la mesa está lista o necesita algo más (el estado del dispositivo).
                    </div>
                    <br>
                    
                    <!-- 3- Comunicación con los dispositivos -->
                    <b style="color: rgb(216, 216, 79);">3- Comunicación con los dispositivos</b><br>
                    El módulo de E/S necesita hablar el "idioma" de cada dispositivo (como impresoras, discos duros, teclados, etc.) para poder controlarlos. Debe ser capaz de reconocer y generar las direcciones asociadas a los dispositivos que controla.<br><br>
                    
                    <div class="contenedor-imagen-tabla">
                        <h4>Ejemplo:</h4> Es como tener un control remoto universal que puede hablar con diferentes aparatos electrónicos, ya sea una televisión, una radio o un aire acondicionado. El módulo de E/S debe saber cómo manejar cada dispositivo de forma correcta. Por ejemplo, cuando el procesador ordena al módulo que envíe un documento a la impresora, el módulo envía los datos y asegura que la impresora entienda la información.
                    </div>
                    <br>

                    <!-- 4- Almacenamiento temporal de datos-->
                    <b style="color: rgb(214, 164, 71);">4- Almacenamiento temporal de datos</b><br>
                    El módulo de entrada/salida se conecta al resto de la computadora a través de un conjunto de líneas de bus. Los datos que viajan entre el procesador y los dispositivos externos se <b style="color: rgb(214, 164, 71);"> guardan temporalmente</b> en pequeños registros dentro del módulo de E/S, conocidos como registros de datos. Estos registros permiten almacenar los datos antes de enviarlos al dispositivo externo o de recibirlos desde el procesador.
                    
                    <div class="contenedor-imagen-tabla">
                        <h4>Ejemplo:</h4> Imagina un camarero que recibe varios platos de comida para llevar a diferentes mesas. Como no puede llevarlos todos al mismo tiempo, los coloca temporalmente en una bandeja (el "almacenamiento temporal") hasta que pueda entregarlos a las mesas correspondientes.
                    </div>
                    <br>

                    <!-- 5- Detección de errores -->
                    <b style="color: rgb(88, 89, 190);">5- Detección de errores</b><br>
                    Además, puede haber uno o más registros de estado en el módulo de E/S que proporcionan información sobre el estado actual de la comunicación. Estos registros de estado también pueden funcionar como <b style="color: rgb(88, 89, 190);">registros de control</b>, recibiendo información de control del procesador.<br>
                    <b>El módulo de E/S verifica que la comunicación con los dispositivos externos sea correcta, detectando cualquier error que pueda ocurrir durante la transferencia de datos.</b>
                    
                    <div class="contenedor-imagen-tabla">
                        <h4>Ejemplo:</h4> Si un archivo no se transfiere correctamente desde una unidad USB, el módulo de E/S lo detecta y alerta al sistema para que se realice la corrección necesaria.<br>
                        <u>Otra Analogía:</u> imagina que estás enviando un mensaje de texto y parte de este no llega correctamente. El teléfono, actuando como un módulo de E/S, se da cuenta del error y reenvía la parte que faltó para asegurarse de que el mensaje llegue completo.                
                    </div>
                </p>
                <br>

                <h3>Técnicas de entrada/salida: Existen tres técnicas posibles para las operaciones de entrada/salida:</h3>
                <ul class="lista">
                    <li><b>Entrada/Salida programada:</b> Los datos se intercambian entre el procesador y el módulo de entrada/salida. El procesador ejecuta un programa que controla directamente la operación de entrada/salida, incluyendo la comprobación del estado del dispositivo, el envío de una orden de lectura o escritura y la transferencia de datos. Cuando el procesador envía una orden al módulo de entrada/salida, debe esperar hasta que la operación de entrada/salida se complete.<br><br>
                        Ejemplo: Supón que estás haciendo cola en una tienda. Cada vez que haces una compra, debes esperar tu turno y no puedes hacer nada más mientras tanto. El procesador debe esperar a que termine la E/S antes de continuar con otras tareas.</li>
                    <li><b>Entrada/Salida mediante interrupciones:</b> El procesador proporciona la orden de entrada/salida y continúa ejecutando otras instrucciones. El módulo de entrada/salida interrumpe al procesador cuando ha terminado su trabajo. Tanto en la entrada/salida programada como en la entrada/salida mediante interrupciones, el procesador es responsable de extraer los datos de la memoria principal para una operación de salida y de almacenar los datos en la memoria principal para una operación de entrada.<br><br>
                        Ejemplo: Imagínate que estás cocinando y mientras esperas a que hierva el agua, te pones a limpiar la cocina. El agua hierve (la E/S termina) y la olla silba (interrupción) para avisarte que está lista, así puedes seguir con lo que estabas haciendo.</li>
                    <li><b>DMA (Acceso Directo a Memoria):</b> Es una alternativa en la que el módulo de entrada/salida y la memoria principal intercambian datos directamente, sin la intervención del procesador. En esta técnica, el procesador solo inicia la transferencia y es liberado para realizar otras tareas mientras la operación de entrada/salida se lleva a cabo de manera autónoma por el módulo de DMA.<br><br>
                        Ejemplo: Es como delegar tareas a un asistente de confianza. El asistente se encarga de toda la tarea (transfiriendo datos directamente), y solo te avisa cuando ya está hecho. Así tú (el procesador) puedes concentrarte en otras cosas sin estar involucrado en cada detalle.</li>
                </ul>
                <br>

                <h4>Técnicas de entrada/salida (más a detalle)</h4>
                <h3>E/S programadas:</h3>
                <p>
                    Cuando el procesador está ejecutando un programa y encuentra una instrucción relacionada con una entrada/salida, ejecuta dicha instrucción enviando una orden al módulo de entrada/salida apropiado. Con entrada/salida programada, el módulo de entrada/salida realizará la acción solicitada y después activará los bits apropiados en el registro de estado de entrada/salida. El módulo de entrada/salida no realiza ninguna otra acción para avisar al procesador. En concreto, no interrumpe al procesador. De esta forma, el procesador es responsable de comprobar periódicamente el estado del módulo de entrada/salida hasta que encuentre que la operación ha terminado.<br>
                    Al ejecutar una instrucción relacionada con una entrada/salida, el procesador proporciona una dirección especificando el módulo de entrada/salida particular y el dispositivo externo, así como una orden de entrada/salida. Hay 4 tipos de órdenes:<br>
                    <ul class="lista">
                        <li><b>Control:</b> Se utiliza para activar el periférico e indicarle qué hacer.</li>
                        <li><b>Test:</b> Se utiliza para comprobar diversas condiciones de estado asociadas con el módulo de entrada/salida y sus periféricos.</li>
                        <li><b>Lectura:</b> Hace que el módulo capte un dato de un periférico y lo sitúe en el buffer interno.</li>
                        <li><b>Escritura:</b> Hace que el módulo de entrada/salida capte un dato del bus de datos y lo transmita al periférico.</li>
                    </ul>
                    <br>

                    Normalmente, habrá muchos dispositivos de entrada/salida conectados al sistema a través de los módulos de entrada/salida. Cada dispositivo tiene asociado un identificador único o dirección. Cuando el procesador envía una orden de entrada/salida, la orden contiene la dirección del dispositivo deseado. Así, cada módulo de entrada/salida debe interpretar las líneas de dirección para determinar si la orden es para él. Cuando el procesador, la memoria principal y las entradas/salidas comparten un bus común, son posibles dos modos de direccionamiento:<br>
                    <ul class="lista">
                        <li><b>Asignado en memoria:</b> Existe un único espacio de direcciones para las posiciones de memoria y los dispositivos de entrada/salida. El procesador considera los registros de estado y de datos de los módulos de entrada/salida como posiciones de memoria y utiliza las mismas instrucciones máquina para acceder tanto a la memoria como a los dispositivos de entrada/salida.</li>
                        <li><b>Aislado:</b> Con las entradas/salidas asignadas en memoria se necesitará una sola línea de lectura y una sola de escritura en el bus. Alternativamente, el bus puede disponer de líneas de lectura y escritura en memoria junto con líneas para órdenes de entrada y salida. En este caso, las líneas de órdenes especifican si las direcciones se refieren a una posición de memoria o a un dispositivo de entrada/salida. Como el espacio de direcciones de entrada/salida está aislado del de memoria, este modo de direccionamiento se conoce como 'entrada/salida aislada'.</li>
                    </ul>
                </p>
                <br>

                <h3>E/S con interrupciones:</h3>
                <p>
                    El problema de la entrada/salida programada es que el procesador tiene que esperar un tiempo considerable a que el módulo de entrada/salida en cuestión esté preparado para recibir o transmitir los datos. Mientras espera, el procesador debe comprobar repetidamente el estado del módulo de entrada/salida. Como consecuencia, se degrada el nivel de prestaciones de todo el sistema.<br>
                    Una alternativa consiste en que el procesador, tras enviar una orden de entrada/salida a un módulo, continúe realizando algún trabajo útil. Después, el módulo de entrada/salida interrumpirá al procesador para solicitar su servicio cuando esté preparado para intercambiar datos con él. El procesador entonces ejecuta la transferencia de datos, como antes, y después continúa con el procesamiento previo.<br>
                    Supongamos que un módulo de entrada/salida recibe una orden de lectura del procesador. Entonces, el módulo de entrada/salida procede a leer el dato desde el periférico asociado. Una vez que el dato está en el registro de datos del módulo, éste envía una interrupción al procesador a través de una línea de control. Luego, espera a que el procesador solicite su dato. Cuando ha recibido la solicitud, el módulo sitúa su dato en el bus de datos y pasa a estar preparado para otra operación de entrada/salida.<br>
                    Desde el punto de vista del procesador, las acciones para una entrada son: el procesador envía una orden de lectura y pasa a hacer otro trabajo. Al final de cada ciclo de instrucciones, el procesador comprueba las interrupciones. Cuando se pide la interrupción desde el módulo de entrada/salida, el procesador guarda el contenido (el PC y los demás registros) del programa actual y procesa la interrupción (lee la palabra de datos del módulo de entrada/salida y la almacena en memoria). Luego recupera el contexto y continúa con su ejecución.<br>
                    En la implementación de las entradas/salidas mediante interrupciones aparecen dos cuestiones. Primero, como que casi invariablemente habrá múltiples módulos de entrada/salida, ¿cómo determina el procesador qué dispositivo ha provocado la interrupción? Y segundo, si se producen varias, ¿cómo decide el procesador las que debe atender? Consideremos en primer lugar la identificación del dispositivo. <b>Se utilizan 4 técnicas comúnmente:</b>
                    <ul class="lista">
                        <li><b>Múltiples líneas de interrupción:</b> poco práctico por la posible gran cantidad de módulos.</li>
                        <li><b>Consulta software (polling):</b> al detectar una interrupción, se produce una bifurcación a una rutina de servicio de interrupción que se encarga de consultar a cada módulo de entrada/salida para determinar cuál la eligió.</li>
                        <li><b>Conexión en cadena:</b> como la consulta software consume mucho tiempo, surge la conexión en cadena de los módulos de entrada/salida que nos proporciona una consulta hardware. Entonces, cuando el procesador recibe una interrupción, activa el reconocimiento de interrupción. Esta señal se propaga a través de la secuencia de módulos hasta que llega al que solicitó la interrupción. Este módulo responde colocando una palabra en las líneas de datos, denominada "vector", y que es la dirección del módulo de entrada/salida o algún otro tipo de identificador específico. El procesador utiliza el vector como un puntero a la rutina de servicio de dispositivo apropiada. Esta técnica se conoce como "interrupción vectorizada".</li>
                        <li><b>Arbitraje de bus:</b> con el arbitraje de bus, un módulo de entrada/salida debe en primer lugar disponer del control del bus antes de poder activar la línea de petición de interrupción. Así, sólo un módulo puede activar la línea en un instante. Cuando el procesador detecta la interrupción, responde mediante la línea de reconocimiento de interrupción. Después, el módulo que solicitó la interrupción sitúa su vector en las líneas de datos.</li>
                    </ul>
                </p>
                <br>

                <h3>Acceso Directo a Memoria (DMA):</h3>
                <p>
                    La entrada/salida con interrupciones, aunque más eficiente que la programada, también requiere la intervención activa del procesador para transferir datos entre la memoria y el módulo de entrada/salida, y cualquier transferencia de datos debe seguir un camino a través del procesador.<br>
                    Cuando hay que transferir grandes volúmenes de datos, se requiere una técnica más eficiente: el acceso directo a memoria (DMA). El DMA requiere un módulo adicional en el bus del sistema. Dicho módulo es capaz de imitar al procesador y, de hecho, de recibir el control del sistema cedido por el procesador (necesario para transferir datos a, y desde, la memoria a través del bus del sistema). Para hacerlo, el módulo de DMA debe utilizarse sólo cuando el procesador no lo necesita, o debe forzar al procesador a que suspenda temporalmente su funcionamiento. Esta última técnica es la más común y se denomina "robo de ciclo" puesto que, en efecto, el módulo de DMA roba un ciclo del bus.<br>
                    Cuando el procesador desea leer o escribir un bloque de datos, envía una orden al módulo de DMA, incluyendo la siguiente información:
                    <ul class="lista">
                        <li><b>Si se solicita una lectura o escritura,</b> utilizando la línea de control correspondiente.</li>
                        <li><b>Dirección del dispositivo de entrada/salida,</b> indicada a través de líneas de datos.</li>
                        <li><b>Posición inicial de memoria</b> a partir de la que se lee o escribe, indicada a través de las líneas de datos y almacenada por el módulo de DMA en su registro de direcciones.</li>
                        <li><b>Número de palabras</b> a leer o escribir, almacenado en el registro de cuenta de datos e indicado a través de las líneas de datos.</li>
                    </ul>
                    <br>

                    Después, el procesador continúa con otro trabajo. Ha delegado la operación de entrada/salida al módulo de DMA, que se encargará de ella. El módulo de DMA transfiere el bloque completo de datos, palabra a palabra, directo desde o hacia la memoria, sin que tenga que pasar a través del procesador. Cuando la transferencia se ha terminado, el módulo de DMA envía una señal de interrupción al procesador. De esta forma, el procesador solo interviene al comienzo y al final de la transferencia.<br>
                    No se trata de una interrupción de la acción en sí; el procesador no guarda el contexto ni hace nada más cuando el módulo de DMA le roba el ciclo. En cambio, espera durante un ciclo de bus. El efecto resultante es que el procesador es más lento ejecutando programas. El número de ciclos de bus necesarios puede reducirse sustancialmente si se integran las funciones de DMA y entrada/salida.<br>
                    Esto significa que existe un camino entre el módulo de DMA y uno o más módulos de Entrada/salida que no incluye al bus del sistema. La lógica de DMA puede ser parte de un módulo de entrada/salida, o puede ser un módulo separado que controla a uno o más módulos de entrada/salida. Este concepto se puede llevar algo más lejos conectando los módulos de entrada/salida al DMA mediante un bus de entrada/salida en el módulo de entrada/salida y produce una configuración fácilmente ampliable.<br>
                    En todos estos casos el bus del sistema, que el módulo de DMA comparte con el procesador y la memoria, es usado por el módulo de DMA solo para intercambiar datos con la memoria. El intercambio de datos entre los módulos de DMA y entrada/salida se produce fuera del bus del sistema.
                </p>
                
                <button onclick="scrollToTop('entrada-salida')">▲</button>
            </section>

            <!-- ********************************************************************************************************** PARALELISMO -->
            <section id="paralelismo" class="contenido">
                <h2 class="titulo2">Paralelismo</h2>
                <p>
                    El <b>paralelismo</b> es la capacidad de realizar múltiples tareas o ejecutar varias operaciones al mismo tiempo.<br> 
                    Su objetivo es reducir el tiempo de ejecución de un programa al dividirlo en partes que se procesan simultáneamente, mejorando así la eficiencia y el uso de los recursos del sistema.
                    <br><br>
                    Existen dos tipos principales de organización de sistemas con múltiples procesadores: los <b>sistemas fuertemente acoplados</b> y los <b>débilmente acoplados</b>. La forma en que los procesadores se comunican y coordinan entre sí afecta la manera en que se logra el paralelismo.
                    <ul class="lista">
                        <li>
                            En un <b>sistema fuertemente acoplado</b>, varios procesadores comparten una memoria principal y un reloj global. Esto permite una comunicación rápida entre ellos, ya que todos acceden a la misma memoria.<br>
                            Este tipo de sistema es común en CPUs multinúcleo o servidores multiprocesador. En este contexto, el paralelismo es fino, lo que significa que las tareas suelen requerir una sincronización constante. Por ejemplo, en un programa que simula fenómenos físicos, cada núcleo realiza una parte del cálculo y accede a la memoria compartida para intercambiar resultados con otros núcleos.
                        </li>
                        <li>
                            Un <b>sistema débilmente acoplado</b>, cada procesador tiene su propia memoria y su propio reloj. La comunicación entre procesadores es más lenta y se realiza mediante el envío de mensajes o a través de redes.<br> 
                            Estos sistemas son típicos en clusters de servidores o sistemas distribuidos. En este caso, el paralelismo es más grueso, ya que las tareas tienden a ser más independientes y no requieren tanta comunicación constante. Un ejemplo sería una simulación climática, donde cada servidor procesa datos de una región específica y sólo ocasionalmente intercambia información con otros servidores.
                        </li>
                    </ul>
                </p>
                <br>

                <h3 class="titulo3">Clasificación de Flynn (Michael J. Flynn - 1966)</h3>
                <div class="texto-desarrollo">
                    <p>
                        La <b>Clasificación de Flynn</b> está directamente relacionada con el paralelismo porque establece cómo una arquitectura de computadora puede manejar múltiples instrucciones y datos simultáneamente, lo que es esencial para entender las diferentes formas en que se implementa el procesamiento paralelo.<br>
                        Flynn identifica cuatro categorías basadas en dos dimensiones: el <u>flujo de instrucciones</u> y el <u>flujo de datos</u>. Estas categorías muestran las distintas maneras en que un sistema puede ejecutar tareas en paralelo, desde arquitecturas más simples hasta complejos sistemas distribuidos.
                        
                        <!-- SSID (Single Instruction, Single Data) -->
                        <h4 class="titulo4">SSID (Single Instruction, Single Data)</h4>
                        <p>
                            <p class="cuadro-chico">
                                Flujo de Instrucciones: 1<br>
                                Flujo de Datos: 1<br>
                                Aquí, el procesador ejecuta <b>una sola instrucción</b> en <b>un conjunto de datos</b> por vez, de manera completamente secuencial. No hay paralelismo.
                            </p>
                            
                            En la arquitectura <b>SISD</b>, una única unidad de procesamiento (CPU) ejecuta <b>una instrucción a la vez</b> sobre un conjunto de datos. Es como seguir una lista de tareas paso a paso, donde cada paso se hace en orden, sin que se pueda hacer nada al mismo tiempo.
                            <ul class="lista">
                                <li><b>No hay paralelismo:</b> Todo sucede de forma secuencial, una operación tras otra, sin simultaneidad.</li>
                                <li><b>Estructura simple:</b> Este tipo de diseño es más fácil de programar y entender porque no se necesitan mecanismos complejos para coordinar varios procesadores o datos.</li>
                                <li><b>Limitaciones:</b> El rendimiento está limitado por la velocidad del único procesador y por la rapidez con la que se transfieren los datos. Si se necesita mucho cálculo, puede volverse lento.</li>
                            </ul>
                            
                            <div class="contenedor-imagen-tabla">
                                <u>Ejemplo sencillo:</u><br>
                                Imagina que eres un chef trabajando solo en la cocina. Tu tarea es hacer un sandwich siguiendo estos pasos:
                                <ol>
                                    <li>Tomas el pan.</li>
                                    <li>Colocas una rebanada de queso.</li>
                                    <li>Añades jamón.</li>
                                    <li>Cierras el sandwich.</li>
                                </ol>
                                Cada acción se hace <b>una por vez</b>, y no puedes empezar un paso nuevo hasta que termines el anterior. No hay nadie más que pueda ayudarte ni tampoco puedes hacer dos cosas a la vez, como cortar el pan mientras buscas el queso.
                                <br><br>
                                <u>Relación con computadoras:</u><br>
                                Una PC tradicional con un solo núcleo de procesador funciona como ese chef. Ejecuta cada instrucción del programa en orden, paso a paso, hasta completarlo. Este enfoque es adecuado para tareas simples que no necesitan mucha velocidad ni múltiples procesos al mismo tiempo, como un editor de texto o una calculadora.
                            </div>
                            <br>
                            
                            Este modelo es adecuado para tareas que no requieren procesamiento paralelo y es la base de muchos sistemas computacionales tradicionales. Sin embargo, su rendimiento puede ser insuficiente para aplicaciones modernas que demandan mayor velocidad y eficiencia.
                        </p>


                        <!-- SIMD (Single Instruction, Multiple Data) -->
                        <h4 class="titulo4">SIMD (Single Instruction, Multiple Data)</h4>
                        <p>
                            <p class="cuadro-chico">
                                Flujo de Instrucciones: 1<br>
                                Flujo de datos: Múltiple<br>
                                En esta arquitectura, <b>una única instrucción</b> se aplica simultáneamente a <b>múltiples datos</b>. Es útil para operaciones que involucran matrices o vectores, como en aplicaciones gráficas o científicas.
                            </p>
                            En la arquitectura <b>SIMD</b>, una misma instrucción se aplica <b>simultáneamente a varios conjuntos de datos</b>. Esto es útil cuando se deben realizar las mismas operaciones repetidas en grandes volúmenes de datos, como en gráficos o procesamiento de audio.
                            <ul class="lista">
                                <li><b>Paralelismo de datos:</b> Aprovecha la capacidad de trabajar con varios datos al mismo tiempo aplicando la misma instrucción a todos ellos.</li>
                                <li><b>Sincronización:</b> Todos los procesadores ejecutan la misma operación al mismo tiempo sobre diferentes datos.</li>
                                <li><b>Eficiencia:</b> Es ideal para tareas que implican operaciones matemáticas repetitivas, como la manipulación de vectores o matrices.</li>
                                <li><b>Limitaciones:</b> Menos flexible, ya que todas las unidades procesan la misma instrucción. Si se necesita hacer algo diferente para un dato en particular, no será posible.</li>
                            </ul>
                            
                            <div class="contenedor-imagen-tabla">
                                <u>Ejemplo sencillo:</u><br>
                                Imagina que eres un entrenador en un gimnasio, y tu tarea es dirigir un grupo de 10 personas para que hagan el <b>mismo ejercicio al mismo tiempo</b>.
                                <ol>
                                    <li>Das la instrucción: "¡Hagan 10 flexiones de brazos!"</li>
                                    <li>Todos los participantes comienzan a hacer flexiones a la vez.</li>
                                </ol>
                                Cada persona es como un conjunto de datos, y tú como entrenador das una instrucción única que todos siguen simultáneamente. No puedes dar instrucciones diferentes a cada persona; todos deben seguir la misma.
                                <br><br>
    
                                <u>Relación con computadoras:</u><br>
                                Este es el tipo de funcionamiento que usan las GPU (unidades de procesamiento gráfico). Por ejemplo, al renderizar una imagen, se realiza la misma operación (como aplicar un color) sobre muchos píxeles simultáneamente.
                            </div>
                            <br>
                            Esta arquitectura es ideal para aplicaciones que pueden aprovechar el paralelismo de datos para mejorar significativamente el rendimiento, especialmente en tareas intensivas en cálculo donde la misma operación debe aplicarse repetidamente a grandes conjuntos de datos.
                        </p>


                        <!-- MISD (Multiple Instruction, Single Data) -->
                        <h4 class="titulo4">MISD (Multiple Instruction, Single Data)</h4>
                        <p>
                            <p class="cuadro-chico">
                                Flujo de Instrucciones: Múltiple<br>
                                Flujo de datos: 1<br>
                                <b>Varias instrucciones</b> distintas se ejecutan sobre <b>un mismo conjunto de datos</b>. Este tipo de arquitectura es poco común, pero se utiliza en sistemas de control críticos, donde diferentes algoritmos analizan los mismos datos.
                            </p>

                            En la arquitectura <b>MISD</b>, varias unidades de procesamiento aplican <b>diferentes instrucciones al mismo dato de forma simultánea</b>. Esta configuración es poco común porque su implementación es compleja y tiene aplicaciones muy específicas.

                            <ul class="lista">
                                <li><b>Paralelismo de instrucciones:</b> Varias instrucciones distintas operan sobre el mismo dato simultáneamente.</li>
                                <li><b>Aplicaciones específicas:</b> Se utiliza en sistemas de alta fiabilidad, donde el mismo dato se procesa varias veces de distintas maneras para obtener redundancia y evitar errores.</li>
                                <li><b>Complejidad:</b> Es difícil de implementar y no tiene ventajas claras en la mayoría de los contextos cotidianos.</li>
                                <li><b>Ejemplo:</b> Se puede encontrar en sistemas de tolerancia a fallos o procesamiento crítico, como en naves espaciales o entornos donde la seguridad es esencial.</li>
                            </ul>

                            <div class="contenedor-imagen-tabla">
                                <u>Ejemplo sencillo:</u><br>
                                Imagina que un equipo de médicos recibe el mismo análisis de sangre de un paciente. Cada médico hace un diagnóstico diferente usando su propia técnica o método: uno busca infecciones, otro chequea niveles hormonales, y otro analiza la presencia de toxinas. Aunque todos trabajan sobre la misma muestra, aplican procedimientos distintos para garantizar que el diagnóstico sea lo más preciso posible.
                                <br><br>
        
                                <u>Relación con computadoras:</u><br>
                                Este tipo de arquitectura se utiliza en sistemas críticos, donde varios procesadores realizan operaciones distintas sobre el mismo dato para detectar cualquier fallo. Un ejemplo serían los sistemas de control de una aeronave o nave espacial, donde la seguridad es tan importante que deben repetirse los cálculos bajo diferentes métodos para confirmar que los resultados sean correctos.
                            </div><br>
                            
                            Esta arquitectura es ideal para entornos que requieren una alta fiabilidad y tolerancia a fallos, aunque su complejidad limita su uso en aplicaciones más comunes.
                        </p>


                        <!-- MIMD (Multiple Instruction, Multiple Data) -->
                        <h4 class="titulo4">MIMD (Multiple Instruction, Multiple Data)</h4>
                        <p>
                            <p class="cuadro-chico">
                                Flujo de Instrucciones: Múltiple<br>
                                Flujo de datos: Múltiple<br>
                                Aquí, <b>múltiples procesadores ejecutan instrucciones</b> distintas sobre <b>diferentes conjuntos de datos simultáneamente</b>. Este modelo es común en sistemas distribuidos o multiprocesadores modernos, como supercomputadoras.
                            </p>
                            
                            En una arquitectura <b>MIMD</b>, varios procesadores independientes ejecutan <b>diferentes instrucciones</b> sobre diferentes conjuntos de datos de manera simultánea.<br>
                            Es el modelo más general y flexible, predominante en sistemas multiprocesadores y supercomputadoras modernas.
                            <ul class="lista">
                                <li><b>Paralelismo Total:</b> Explota tanto el paralelismo de datos como el de instrucciones.</li>
                                <li><b>Flexibilidad:</b> Cada procesador ejecuta su propio programa y gestiona su conjunto de datos.</li>
                                <li>
                                    <b>Arquitecturas de Memoria:</b>
                                    <ul class="lista">
                                        <li><u>Memoria compartida:</u> Procesadores acceden a una memoria común.</li>
                                        <li><u>Memoria distribuida:</u> Cada procesador tiene su memoria privada y se comunica mediante mensajes.</li>
                                    </ul>
                                </li>
                                <li><b>Eficiencia:</b> Ideal para dividir tareas en subtareas independientes.</li>
                            </ul>

                            <div class="contenedor-imagen-tabla">
                                <u>Ejemplos de Uso:</u><br>
                                <ul class="lista">
                                    <li>Supercomptuadoras modernas.</li>
                                    <li>Sistemas de servidores de alto rendimiento.</li>
                                    <li>Clusters de computadoras, empleados para computación científica, análisis de grandes volúmenes de datos, e inteligencia artificial.</li>
                                </ul> 
                            </div>
                            <br>
                            
                            Esta arquitectura es ideal para aplicaciones que requieren un alto rendimiento y capacidad de procesamiento paralelo, siendo fundamental en áreas como la computación científica, el análisis de grandes volúmenes de datos y las aplicaciones de inteligencia artificial.<br>
                            
                            <h4 class="titulo4" style="display: inline-block; border-bottom: 2px solid #63880d; font-style: italic;">Tipos de MIMD en Detalle</h4>
                            <p>
                                <b>Multicomputadoras (Memoria Privada o Distribuida):</b><br>
                                Las CPUs (Unidades Centrales de Procesamiento) independientes realizan diferentes trabajos sobre diferentes conjuntos de datos simultáneamente.<br>
                                Cada CPU tiene su memoria privada.<br>
                                La comunicación entre CPUs se realiza mediante mensajes (acoplamiento débil).<br>
                                <ul class="lista">
                                    <li>
                                        <b>Divisiones:</b>
                                        <ul class="sub-item">
                                            <li>
                                                <b>MPP (Massively Parallel Processors):</b> 
                                                <ul class="sub-item">
                                                    <li>Estructura: Organizados en cuadrículas o hipercubos.</li>
                                                    <li>Uso: Cálculos científicos y tareas intensivas en datos.</li>
                                                    <li>Ventajas: Alta capacidad de procesamiento.</li>
                                                    <li>Desventajas: Costosos y complejos de programar.</li>
                                                </ul>
                                            </li>
                                        </ul>
                                    </li>
                                    <br>
                                    <li>
                                        <b>COW (Clusters de Estación de Trabajo):</b>
                                        <ul class="sub-item">
                                            <li><b>Estructura:</b> Computadoras independientes conectadas por red.</li>
                                            <li>
                                                <b>Ventajas:</b>
                                                <ul class="sub-item">
                                                    <li><b>Escalabilidad alta: </b>Se pueden añadir más nodos.</li>
                                                    <li><b>Tolerancia a fallos:</b> Cada nodo es independiente.</li>
                                                </ul>
                                            </li>
                                            <li>
                                                <b>Desventajas:</b>
                                                    <ul class="sub-item">
                                                        <li>Comunicación más lenta por mensajes.</li>
                                                        <li>Red y latencia pueden afectar la eficiencia.</li>
                                                    </ul>
                                            </li>
                                        </ul>
                                    </li>
                                </ul>
                                

    
                                <b>Multiprocesadores (Memoria Compartida)</b><br>
                                Las CPUs están fuertemente acopladas y comparten una <b>memoria común</b> y se comunican a través de ella.<br> 
                                Permiten la comunicación directa y rápida entre procesadores a través de la memoria compartida.<br> 
                                Se divide en <b>SMP (UMA [Acceso a Memoria Uniforme])</b> y <b>NUMA (Acceso a Memoria No Uniforme).</b>
                                <ul class="lista">
                                    <li>
                                        <b>Divisiones:</b>
                                        <ul class="sub-item">
                                            <li>
                                                <b>SMP (Symmetric Multiprocessing - Multiprocesadores Simétricos):</b> 
                                                <ul class="sub-item">
                                                    <li><b>Estructura:</b> Dos o más procesadores idénticos.</li>
                                                    <li><b>Interconexión:</b> Mediante un bus compartido.</li>
                                                    <li>
                                                        <b>Ventajas:</b>
                                                        <ul class="sub-item">
                                                            <li>Comunicación rápida y sincronización sencilla.</li>
                                                            <li>Facilita la programación.</li>
                                                        </ul>
                                                    </li>
                                                    <li>
                                                        <b>Desventajas:</b> 
                                                        <li>Competencia por el bus, limitando el número de procesadores.</li>
                                                        <li>Cuello de botella al acceder a la memoria principal.</li>
                                                    </li>
                                                </ul>
                                            </li>
                                        </ul>
                                    </li>
                                    <br>
                                    <li>
                                        <b>NUMA (Non-Uniform Memory Access):</b>
                                        <ul class="sub-item">
                                            <li><b>Estructura:</b> Cada procesador tiene acceso a una memoria <b>local</b> más rápida.</li>
                                            <li>
                                                <b>Ventajas:</b>
                                                <ul class="sub-item">
                                                    <li>Recude el cuello de botella en el acceso a la memoria princial.</li>
                                                    <li>Reduce la latencia y mejora el rendimiento.</li>
                                                </ul>
                                            </li>
                                            <li>
                                                <b>Desventajas:</b>
                                                    <ul class="sub-item">
                                                        <li>La gestión de memoria es compleja y requiere protocolos avanzados para coherencia.</li>
                                                    </ul>
                                            </li>
                                        </ul>
                                    </li>
                                </ul>
                            </p>
                        </p>

                        <div class="contenedor-imagen-tabla">
                            <table>
                                <tr>
                                    <th style="background-color: #63880d;"> </th>
                                    <th>Multicomputadoras</th>
                                    <th>Multiprocesadores</th>
                                </tr>
                                <tr>
                                    <td class="fila-descripcion"><b>Memoria</b></td>
                                    <td> Usan memoria privada y se comunican a través de mensajes.</td>
                                    <td>Usan memoria compartida</td>
                                </tr>
                                <tr>
                                    <td class="fila-descripcion"><b>Escalabilidad</b></td>
                                    <td>Alta escalabilidad debido a la independencia de nodos.</td>
                                    <td>Limitada escalabilidad por el cuello de botella en el acceso a la memoria</td>
                                </tr>
                                <tr>
                                    <td class="fila-descripcion"><b>Comunicación</b></td>
                                    <td>Más lenta debido a la transparencia de mensajes.</td>
                                    <td> Cálculos científicos y aplicaciones de alto rendimiento que requieren alta escalabilidad.</td>
                                </tr>
                                <tr>
                                    <td class="fila-descripcion"><b>Aplicaciones</b></td>
                                    <td>Cálculos científicos y aplicaciones de alto rendimiento que requieren alta escalabilidad.</td>
                                    <td>Servidores y sistemas de bases de datos que requieren accesos rápido y simultáneo a grandes cantidades de datos compartidos.</td>
                                </tr>
                            </table>
                        </div>

                    </p>
                </div>

                <h3 class="titulo3">Memoria Privada vs. Memoria Compartida</h3>
                <div class="texto-desarrollo">
                    <p>
                        desarrollo.
                    </p>
                </div>

                <h3 class="titulo3">Interconexión: Bus vs. Conmutado</h3>
                <div class="texto-desarrollo">
                    <p>
                        desarrollo.
                    </p>
                </div>










                <button onclick="scrollToTop('paralelismo')">▲</button>
            </section>

            <!-- ********************************************************************************************************** SECUENCIADORES -->
            <section id="secuenciadores" class="contenido">
                <h2>SECUENCIADORES</h2>
                <p>Descripción detallada sobre los sistemas de SECUENCIADORES</p>
                <button onclick="scrollToTop('secuenciadores')">▲</button>
            </section>

            

            <!-- ********************************************************************************************************** MEMORIA CACHE -->
            <section id="memoria-cache" class="contenido">
                <h2 class="titulo2"> MEMORIA CACHÉ (MC)</h2>
                <p>
                    Es un tipo de memoria de <u>acceso rápido</u> que se encuentra entre la CPU y la memoria principal de una computadora. Su función principal es almacenar temporalmente datos e instrucciones que se utilizan con frecuencia, lo que ayuda a acelerar el acceso a ellos y mejorar el rendimiento del sistema.<br>
                    El paradigma principal en que se basa el funcionamiento de la memoria caché es el principio de <b>localidad de referencia</b>.
                    <br><br>
                    Este principio se divide en:
                    <ul class="lista">
                        <li><b>Localidad temporal:</b> Los datos a los que se ha accedido recientemente es probable que se vuelvan a necesitar en un futuro cercano.</li>
                        <li><b>Localidad espacial:</b> Los datos cercanos en memoria a los que ha accedido recientemente es probable que se necesiten pronto.</li>
                    </ul>
                    <br>
                    La mejora efectiva que se logra usando la memoria caché es la <u>reducción del tiempo promedio de acceso a la memoria</u>, lo que incrementa el rendimiento del sistema. Esto se debe a que la caché almacena temporalmente los datos más frecuentes utilizados, permitiendo que el procesador acceda a estos de manera mucho más rápida que si tuviera que acceder a la memoria principal (RAM) cada vez.<br>
                    La memoria caché es principalmente una <b>implementación de hardware</b>.  Sin embargo, su eficacia también puede ser mejorada con estrategias de software que optimicen el uso de la caché, como las técnicas de programación que promueven la <u>localización de referencia</u>.<br>
                </p>
                <br>

                <h3>Función de mapeo:</h3>
                <p>
                    Es el mecanismo fundamental que determina cómo los bloques de memoria principal se asignan a las líneas de la memoria caché.<br> 
                    Esta función es crucial para el funcionamiento eficiente de la memoria caché por las siguientes razones:
                    <ol>
                        <li><b>Asignación de bloques:</b> Define cómo se distribuyen los datos de la memoria principal en la estructura más pequeña de la caché.</li>
                        <li><b>Optimización del acceso:</b> Busca equilibrar la velocidad de acceso con la utilización eficiente del espacio en caché.</li>
                        <li><b>Gestión de conflictos:</b> Ayuda a manejar situaciones donde múltiples bloques de memoria principal podrían mapear a la misma ubicación en caché.</li>
                        <li><b>Impacto en el rendimiento:</b> La elección de la función de mapeo afecta directamente la tasa de aciertos (hit rate) de la caché, lo que influye en el rendimiento global del sistema.</li>
                        <li><b>Flexibilidad vs. Complejidad:</b> Diferentes funciones de mapeo ofrecen distintos grados de flexibilidad en la asignación, pero también pueden variar en su complejidad de implementación.</li>
                    </ol>
                    <br>

                    La función de mapeo es esencial para determinar:
                    <ul class="lista">
                        <li>Dónde se almacena un bloque específico de memoria principal dentro de la caché.</li>
                        <li>Cómo se localiza un dato cuando el procesador lo solicita.</li>
                        <li>Qué estrategia se utiliza para reemplazar datos en la caché cuando está llena.  </li>
                    </ul>
                </p>
                <br>

                <h4><u>Existen tres principales tipos de funciones de mapeo:</u></h4>
                <ol>
                    <li>Mapeo Directo</li>
                    <li>Mapeo Asociativo</li>
                    <li>Mapeo Asociativo por Conjunto</li>
                </ol>
                <br>

                <h3>Mapeo Directo</h3>
                <p>
                    El Mapeo Directo es una técnica de función de mapeo en la memoria caché que <b>asigna cada bloque de la memoria principal a una única ubicación en la caché</b>.<br>
                    En este esquema, múltiples bloques de memoria principal pueden corresponder a la misma línea de caché, pero solo uno a la vez.<br> 
                    Utiliza una fórmula simple, típicamente (Dirección del bloque) mod (Número de líneas en caché), para determinar la ubicación.<br> 
                    Cada línea de caché se estructura con una etiqueta, un índice y un desplazamiento.<br> 
                    Para acceder a los datos, el índice localiza la línea, la etiqueta verifica el bloque correcto, y el desplazamiento encuentra el byte específico.<br> 
                    <i style="color: rgb(60, 163, 60)">Aunque es rápido y fácil de implementar,</i>  <i style="color: rgb(238, 91, 91);">el Mapeo Directo puede generar conflictos cuando varios bloques compiten por la misma línea, lo que puede resultar en reemplazos frecuentes y una utilización ineficiente del espacio de caché, potencialmente aumentando la tasa de fallos en ciertos patrones de acceso a memoria.</i><br>
                    Este tipo de función de mapeo no requiere de algoritmos de reemplazo.
                </p>
                <br>
                
                <h4>Ventajas</h4>
                <ul class="lista">
                    <li>Es fácil de implementar</li>
                    <li>La búsqueda dentro de la memoria caché es rápida.</li>
                </ul>
                <br>

                <h4>Desventajas</h4>
                <ul class="lista">
                    <li>No se aprovecha la memoria caché al máximo, debido a que se van a reemplazar bloques así sea que esta no está llena.</li>
                    <li>Es muy probable que exista competencia por una misma línea en memoria caché, aun cuando esta no esté llena. <br> Esto va a pasar cuando dos o más bloques tengan asignado la misma línea en meoria caché. Lo que termina reduciendo la tasa de aciertos y provocando miss de caché.</li>
                </ul>
                
                <img src="images/adc-mapeoDirecto.png" alt="Mapeo Directo" style="width: 600px; padding: 30px;">
                
                <h3>Mapeo Asociativo</h3>
                <p>
                    El Mapeo Asociativo es una técnica de función de mapeo en la memoria caché que <b>permite que cualquier bloque de la memoria principal pueda ser almacenado en cualquier línea de la caché.</b><br> 
                    Este enfoque <i style="color: rgb(60, 163, 60);">ofrece máxima flexibilidad, eliminando los conflictos de ubicación presentes en el mapeo directo</i>.<br> 
                    En este esquema, cada línea de caché contiene tanto los datos como una etiqueta que identifica el bloque de memoria principal correspondiente.<br> 
                    Para encontrar un dato, <i style="color: rgb(238, 91, 91);">se debe buscar en toda la caché, comparando la etiqueta de cada línea con la dirección solicitada.</i><br> 
                    <i style="color: rgb(60, 163, 60);">Esto resulta en un uso más eficiente del espacio de caché y una tasa de aciertos potencialmente mayor, especialmente para patrones de acceso a memoria irregulares.</i><br> 
                    <i style="color: rgb(238, 91, 91);">Sin embargo, la búsqueda exhaustiva necesaria para localizar los datos puede aumentar la complejidad del hardware y el tiempo de acceso.</i><br> 
                    El mapeo asociativo es particularmente <i style="color: rgb(60, 163, 60);">útil en cachés pequeñas o en situaciones donde la predicción de patrones de acceso a memoria es difícil</i> pero su implementación <b>es más costosa</b> en términos de hardware comparado con otras técnicas de mapeo.
                </p>
                <br><br>

                <h4>Ventajas:</h4>
                <ul class="lista">
                    <li>Se aprovecha al máximo a la memoria caché, no se reemplaza un bloque de la misma a no ser que esta esté llena.</li>
                    <li>Aumenta la tasa de aciertos.</li>
                </ul>

                <h4>Desventajas:</h4>
                <ul class="lista">
                    <li>La búsqueda en memoria caché es más lenta que en el caso anterior.</li>
                    <li>Se debe recorrer la memoria caché comparando el TAG de cada línea con el TAG de la palabra que se busca.</li>
                </ul>
                <img src="images/adc-mapeoAsociativo.png" alt="Mapeo Asociativo" style="width: 600px; padding: 30px;">

                <h3>Mapeo Asociativo por Conjunto</h3>
                <p>
                    El Mapeo Asociativo por Conjunto es una técnica de administración de la memoria caché que combina las ventajas del mapeo directo y el mapeo totalmente asociativo.<br> 
                    Esta estrategia <b>divide la caché en conjuntos, cada uno conteniendo múltiples líneas.</b> Un bloque de memoria principal se asigna a un conjunto específico (como en el mapeo directo), pero puede ocupar cualquier línea dentro de ese conjunto (como en el mapeo asociativo).<br>
                    <i style="color: rgb(60, 163, 60);">Esta técnica ofrece un equilibrio entre velocidad de acceso y tasa de aciertos. La búsqueda se limita a un conjunto específico, lo que es más rápido que una búsqueda completa de la caché, mientras que la flexibilidad dentro del conjunto reduce los conflictos de ubicación.</i><br>
                    El <b>grado de asociatividad (N)</b> determina cuántas líneas hay en cada conjunto.<br> 
                    Cuando N=1, equivale al mapeo directo; cuando N iguala el número total de líneas en la caché, se convierte en mapeo totalmente asociativo. En la práctica, se elige un valor intermedio de N para optimizar el rendimiento, aprovechando tanto la rapidez de búsqueda del mapeo directo como la alta tasa de aciertos del mapeo asociativo.<br>
                    Esta implementación proporciona un compromiso eficaz entre complejidad de hardware, velocidad de acceso y eficiencia en el uso del espacio de caché, haciéndola una opción popular en muchos sistemas modernos.
                </p>
                <img src="images/adc-mapeoAsociativoPorConjunto.png" alt="Mapeo Asociativo" style="width: 600px; padding: 30px;">


                <h3>Algoritmo de Reemplazo</h3>
                <p>
                    Es una estrategia crucial en la gestión de la memoria caché que determina qué línea de la caché debe ser reemplazada cuando se necesita almacenar un nuevo bloque y la caché está llena. Este algoritmo es fundamental para mantener la eficiencia y el rendimiento de la memoria caché por las siguientes razones:
                </p>
                <ol>
                    <li><b>Optimización del rendimiento:</b> Busca mantener en la caché los datos que tienen más probabilidades de ser utilizados en el futuro cercano.</li>
                    <li><b>Gestión de conflictos:</b> Resuelve situaciones donde múltiples bloques compiten por el mismo espacio en la caché.</li>
                    <li><b>Balanceo de prioridades:</b> Equilibra la necesidad de mantener datos frecuentemente utilizados con la de incorporar nuevos datos potencialmente importantes.</li>
                    <li><b>Impacto en la tasa de aciertos:</b> La elección del algoritmo afecta directamente la tasa de aciertos de la caché, influyendo así en el rendimiento global del sistema.</li>
                    <li><b>Adaptabilidad:</b> Algunos algoritmos pueden adaptarse a diferentes patrones de acceso a memoria para optimizar su eficacia.</li>
                    <li><b>Complejidad vs. Eficacia:</b> Los algoritmos varían en su complejidad de implementación y en su eficacia para diferentes escenarios de uso.</li>
                </ol>
                <br>

                <p>Los algoritmos de reemplazo consideran factores como:</p>
                <ul class="lista">
                    <li>Frecuencia de uso de los bloques</li>
                    <li>Tiempo transcurrido desde el último acceso</li>
                    <li>Patrones de acceso a memoria</li>
                </ul>
                <br>

                <h4><u>Algunos de los algoritmos de reemplazo mas compunes incluyen:</u></h4>
                <div class="contenedor-imagen-tabla">
                    <h3>FIFO (First In, First Out)</h3>
                <p>
                    Es un algoritmo de reemplazo en memoria caché que opera bajo el principio de que el primer bloque que entró en la caché será el primero en ser reemplazado cuando se necesite espacio para un nuevo bloque. Este método trata la caché como una cola, donde los bloques más antiguos se encuentran al frente y son los primeros candidatos para ser eliminados.<br>
                    <b>Funcionamiento:</b> Mantiene un registro del orden en que los bloques fueron cargados en la caché. Cuando se necesita espacio, selecciona para reemplazo el bloque que ha estado en la caché por más tiempo.No considera la frecuencia de uso o el tiempo desde el último acceso a los bloques.<br><br>
                    <h4>Ventaja: </h4>
                    <ul class="lista">
                        <li>Es simple de implementar.</li>
                        <li>Tiene un bajo costo computacional.</li>
                        <li>Overhead mínimo en términos de procesamiento y almacenamiento.</li>
                    </ul>
                    <h4>Desventaja: </h4>
                    <ul class="lista">
                        <li>Puede ser ineficiente si los bloques más antiguos son también los más frecuentemente utilizados.</li>
                        <li>No se adapta a patrones cambiantes de acceso a memoria.</li>
                    </ul>
                </p>
                </div>
                <br>

                <div class="contenedor-imagen-tabla">
                    <h3>LRU (Least Recently Used)</h3>
                    <p>
                        Es un algoritmo de reemplazo en memoria caché que opera bajo el principio de reemplazar el bloque que no ha sido accedido durante el período más largo. Este método prioriza mantener en caché los datos que se han utilizado más recientemente, asumiendo que es más probable que se necesiten de nuevo en un futuro cercano.<br>
                        <b>Funcionamiento:</b> Mantiene un <b>registro del tiempo</b> de último acceso para cada bloque en la caché. Cuando se necesita espacio, selecciona para reemplazo el bloque que tiene el tiempo de último acceso más antiguo. Se actualiza este registro cada vez que se accede a un bloque.<br><br>
                        <h4>Ventajas:</h4>
                        <ul class="lista">
                            <li>Tiende a eliminar los bloques menos utilizados, lo que puede mejorar el rendimiento al mantener en caché los datos más relevantes.</li>
                            <li>Se adapta bien a patrones de acceso a memoria cambiantes.</li>
                            <li>Aprovecha eficazmente el principio de <b>localidad temporal</b>.</li>
                        </ul><br>

                        <h4>Desventajas:</h4>
                        <ul class="lista">
                            <li>Podría tener un overhead de seguimiento, especialmente en sistemas con un gran número de bloques en la caché y un alto volumen de accesos.</li>
                            <li>Requiere hardware más complejo para su implementación comparado con algoritmos más simples como FIFO.</li>
                            <li>Puede ser menos efectivo en situaciones donde el patrón de acceso no sigue el principio de localidad temporal.</li>
                        </ul>
                    </p>
                </div>
                <br>

                <div class="contenedor-imagen-tabla">
                    <h3>LFU (Least Frequently Used)</h3>
                    <p>
                        Es un algoritmo de reemplazo en memoria caché que opera bajo el principio de reemplazar el bloque que ha sido accedido con menor frecuencia. Este método prioriza mantener en caché los datos que se han utilizado más a menudo, asumiendo que los bloques frecuentemente accedidos tienen más probabilidades de ser necesarios en el futuro.<br>
                        <b>Funcionamiento:</b> Mantiene un <b>contador de accesos</b> para cada bloque en la caché. Cuando se necesita espacio, selecciona para reemplazo el bloque que tiene el contador de accesos más bajo. Se incrementa este contador cada vez que se accede a un bloque.<br><br>                    
                        <h4>Ventajas:</h4>
                        <ul class="lista">
                            <li>Elimina los bloques que se han accedido con menos frecuencia, lo que puede ser útil para optimizar el uso de la caché en aplicaciones con patrones de acceso irregulares.</li>
                            <li>Puede ser muy efectivo en situaciones donde ciertos datos son consistentemente más importantes o frecuentemente utilizados que otros.</li>
                            <li>Proporciona una buena medida de la importancia a largo plazo de los bloques de datos.</li>
                        </ul>
                        <br>

                        <h4>Desventajas:</h4>
                        <ul class="lista">
                            <li>Es complicado de implementar, requiriendo hardware adicional para mantener y actualizar los contadores de frecuencia.</li>
                            <li>Puede ser lento para adaptarse a cambios en los patrones de acceso, ya que los bloques con alta frecuencia histórica pueden permanecer en la caché incluso si ya no se utilizan.</li>
                            <li>Puede sufrir del problema de "envejecimiento de caché", donde bloques nuevos pero importantes son expulsados rápidamente debido a sus bajos contadores de frecuencia.</li>
                        </ul>
                    </p>
                </div>
                
                <div class="contenedor-imagen-tabla">
                    <h3>Aleatorio</h3>
                <p>
                    Es un algoritmo de reemplazo en memoria caché que opera bajo el principio de seleccionar al azar un bloque para ser reemplazado cuando se necesita espacio para un nuevo bloque. Este método no considera la frecuencia de uso, el tiempo de permanencia en la caché, ni ningún otro factor para tomar la decisión de reemplazo.<br>
                    <b>Funcionamiento:</b> Cuando se necesita espacio en la caché, simplemente selecciona de manera aleatoria uno de los bloques existentes para ser reemplazado. No requiere mantener ningún tipo de registro o contador asociado a los bloques.<br><br>
                    <h4>Ventajas:</h4>
                    <ul class="lista">
                        <li>Es fácil de implementar, requiriendo muy poco hardware adicional.</li>
                        <li>Es eficiente en términos de tiempo de decisión, ya que no necesita comparar o analizar datos.</li>
                        <li>No tiene sobrecarga (overhead) de mantenimiento de información sobre los bloques.</li>
                        <li>Puede funcionar bien en situaciones donde los patrones de acceso son impredecibles o muy variados.</li>
                    </ul>
                    <br>

                    <h4>Desventajas:</h4>
                    <ul class="lista">
                        <li>El número de bloques debe ser elevado para demostrar la eficacia, ya que con pocos bloques la probabilidad de reemplazar datos importantes aumenta.</li>
                        <li>No garantiza un rendimiento óptimo, ya que puede reemplazar bloques frecuentemente utilizados.</li>
                        <li>Su rendimiento puede ser inconsistente y difícil de predecir.</li>
                        <li>No aprovecha los principios de localidad temporal o espacial que suelen ser beneficiosos en el manejo de caché.</li>
                    </ul>
                </p>
                </div>
                <br>

                <h3>Casos Especiales en la Gestión de Memoria Caché</h3>
                <p>
                    <h4><b>Lectura de una dirección que se encuentra en la Memoria Caché (MC):</b></h4>
                    <ol>
                        <li>El controlador de caché recibe la solicitud de lectura.</li>
                        <li>El controlador busca la línea de caché correspondiente a la dirección solicitada usando la función de mapeo.</li>
                        <li>Verifica el tag de la línea para confirmar que es el dato correcto.</li>
                        <li>Si hay una coincidencia (HIT de caché), los datos son leídos directamente de la caché y enviados al procesador.</li>
                        <li>El acceso es rápido y el tiempo de respuesta es bajo.</li>
                    </ol>
                    <br>
                    
                    <h4><b>Lectura de una dirección que NO se encuentra en la Memoria Caché (MC):</b></h4>
                    <ol>
                        <li>El controlador de caché recibe la solicitud de lectura.</li>
                        <li>El controlador busca la línea de caché correspondiente a la dirección solicitada usando la función de mapeo.</li>
                        <li>Verifica el tag de la línea y detecta que no hay coincidencia (MISS de caché).</li>
                        <li>La solicitud se envía a la memoria principal para leer el bloque de datos requerido.</li>
                        <li>El bloque de datos es cargado en la caché y el tag correspondiente es actualizado.</li>
                        <li>El dato es leído de la caché y enviado al procesador.</li>
                        <li>El acceso es más lento comparado con un HIT en caché debido a la latencia de la memoria principal.</li>
                    </ol>
                    <br>
                    
                    <h4><b>Escritura de una dirección que se encuentra en la Memoria Caché (MC):</b></h4>
                    <ol>
                        <li>El controlador de caché recibe la solicitud de escritura.</li>
                        <li>El controlador busca la línea de caché correspondiente a la dirección solicitada usando la función de mapeo.</li>
                        <li>Verifica el tag de la línea para confirmar que es el dato correcto.</li>
                        <li>Si hay coincidencia (HIT de caché), los datos son escritos directamente a la línea de caché.</li>
                        <li>Dependiendo de la política de escritura, los cambios pueden ser inmediatamente escritos en la memoria principal (write-through) o marcados para escribir más tarde (write-back).</li>
                    </ol><br>

                    <h4><b>Políticas de Escritura:</b></h4>
                    <ul class="lista">
                        <li><b>Write-through:</b> Se escribe de manera simultánea en memoria caché y en memoria principal. Esto da la ventaja de que, al reemplazar ese bloque en memoria caché, simplemente se reemplaza rápidamente. Sin embargo, la desventaja es que cada proceso de escritura tarda más, ya que debe acceder a la memoria principal, lo que puede ralentizar el proceso si hay muchas escrituras.</li>
                        <li><b>Write-back:</b> Se escribe solo en memoria caché y la memoria principal se actualiza solo cuando se va a reemplazar el bloque de memoria caché. La ventaja es que la escritura es rápida, lo cual es recomendable cuando se realizan muchas actualizaciones en los datos. La desventaja es que al reemplazar un bloque de memoria caché, el proceso es más lento, porque se debe reemplazar el bloque completo en la memoria principal.</li>
                    </ul>
                    <br>

                    <h4><b>Escritura de una dirección que NO se encuentra en la Memoria Caché (MC):</b></h4>
                    <ol>
                        <li>El controlador de caché recibe la solicitud de escritura.</li>
                        <li>El controlador busca la línea de caché correspondiente a la dirección solicitada usando la función de mapeo.</li>
                        <li>Verifica el tag de la línea y detecta que no hay coincidencia (MISS de caché).</li>
                        <li>El dato es escrito directamente en la memoria principal sin modificar la caché, ya que no es probable que la próxima escritura sea en el bloque activo, por lo que no vale la pena traerlo a la MC.</li>
                    </ol>

                </p>







                <button onclick="scrollToTop('memoria-cache')">▲</button>
                <!--<button onclick="goBack()">Volver Atrás</button>-->
            </section>

            <!-- ********************************************************************************************************** COHERENCIA CACHE -->
            <section id="coherencia-cache" class="contenido">
                <h2 class="titulo2">COHERENCIA CACHE</h2>
                <p>
                    La coherencia de caché se refiere al problema que surge cuando existe múltiples copias del mismo dato en diferentes cachés y los procesadores actualizan sus copias, lo que puede producir una visión incosistente de la memoria.
                    <br>
                    Según las dos políticas de escritura de caché, <b>Write-Back(WB)</b> y <b>Write-Throug(WT)</b>, la política WB puede causar problemas de coherencia. Si dos cachés contienen la misma línea de datos y una de ellas la actualiza, la otra puede operar con un dato desactualizado. Aunque este problema también puede ocurrir con WT, se puede mitigar si las cachés reciben notificaciones de escritura en la memoria. Es posible usar WT con modificaciones para mejorar la coherencia.
                    <br>
                    Existen dos tipos de soluciones para el problema de coherencia de caché: basadas en software y basadas en hardwares
                </p>
                <br>

                <h3>Soluciones Basadas en Software</h3>
                <p>
                    Las soluciones de software delegan el problema al compilador y al sistema operativo. Estos clasifican los datos como compartidos o no compartidos, y de solo lectura o de lectura-escritura.
                    <ul class="lista">
                        <li><b>Datos no compartidos:</b> No presentan problemas de coherencia ya que son de uso exclusivo del procesador.</li>
                        <li><b>Datos de solo lectura:</b> Tampoco presentan problemas, ya que no se modifican.</li>
                    </ul>
                    El problema surge con los datos compartidos y de escritura. La solución es marcarlos como no-cacheables, evitando que se almacenaen en la caché.
                </p>
                <br>

                <h3>Soluciones Basadas en Hardware</h3>
                <p>
                    Las soluciones de hardware se denominan protocolos de coherencia de cahé. Estas soluciones permiten reconocer dinámicamente en el momento de la ejecución, las situaciones potenciales de inconsistencia. A diferencia de las soluciones basadas en software, estos protocolos gestionan el problema cuando realmente ocurre, optimizando el uso de la caché.
                    <br><br>

                    Existen dos grandes grupos de protocolos: de directorio y de sondeo.
                    <ol>
                        <li><b>Protocolos de Directorio:</b> Recogen y mantienen información sobre dónde residen las copias de las lines de datos. El directorio centralizado contiene el estado global de los contenidos de las diferentes cachés locales.</li>
                        <ul class="lista">
                            <li><b>Ventajas:</b> Control centralizado.</li>
                            <li><b>Desventajas:</b> Cuello de botella en el controlador central y costos de comunicación entre el controlador central y las cachés.</li>
                        </ul><br>
                        <li><b>Protocolos de Sondeos:</b> Distribuyen la responsabilidad de mantener la coherencia de caché entre todos los controladores de caché del multiprocesador. Cada caché debe reconocer cuando posee una linea que está siendo compartida con otras cachés. Cuando una caché modifica una linea, lo anuncia a las demás mediante un mecanismo de difusión (broadcastin). Cada controlador de caché puede sondear la red para observar las notificaciones y reaccioens rápidamente.</li>
                        <ul class="lista">
                            <li><b>Ventajas:</b> Distribución de la carga de mantenimiento de coherencia y rápida reacción a las modificaciones.</li>
                            <li><b>Desventajas:</b> Requiere un mecanismo eficiente de broadcasting.</li>
                        </ul>
                    </ol>
                </p>
                <br>
                
                <h3>Protocolo MESI: (MOESI 🠚 O = Ocupado)</h3>
                <p>
                    Un ejemplo de protocolo de coherencia de caché es el protocolo MESI, donde cada bloque de la caché puede estar en uno de los siguientes estados:
                    <ul class="lista">
                        <li><b>Modificado (M):</b> El bloque ha sido modificado y no es consistente con la memoria principal.</li>
                        <li><b>Exlusivo (E):</b> El bloque está presente solo en esta caché y es consistente con la memoria principal.</li>
                        <li><b>Compartido (S):</b> El bloque está presente en múltiples cachés y es consistentes con la memoria principal.</li>
                        <li><b>Inválido (I):</b> El bloque no contiene datos válidos</li>
                    </ul>
                    <br>
                    Este protocolo mejora la eficienca y la coherencia en sistemas multiprocesador, manejando de manera efectiva las actualizaciones y los estados de los bloques en las cachés.
                </p>
                <button onclick="scrollToTop('coherencia-cache')">▲</button>
                <!--<button onclick="goBack()">Volver Atrás</button>-->
            </section>

            <!-- ********************************************************************************************************** EXAMENES -->
            <section id="examenes" class="contenido">
                <h2>Exámenes</h2>
                <p>
                    Listado de los exámenes recolectados.
                </p>
                <br>

                <!-- MICROCONTROLADORES -->
                <h2 class="titulo2">MICROCONTROLADORES</h2>
                <b class="negrita-color">AÑO 2023</b>
                <ul class="lista">
                    <li>¿Qué son y que los diferencia de los microprocesadores?</li>
                    <li>¿Qué tipo de Arquitectura tienen?</li>
                    <li>Son CISC o RISC? Detalle porqué y que caracteristicas incluye.</li>
                    <li>¿Cómo es el pipeline en los microcontroladores?</li>
                </ul>
                <br>
                
                <b class="negrita-color">AÑO 2024</b>
                <ul class="lista">
                    <li>Describa las principales características de los microcontroladores.</li>
                </ul>
                <br>

                <!-- INTERRUPCIONES-->
                <h2 class="titulo2">INTERRUPCIONES</h2>
                <b class="negrita-color">AÑO 2023 | 2024</b>
                <ul class="lista">
                    <li>Describa el funcionamiento general y su utilidad.</li>
                    <li>Describa en detalle cómo es el mecnaismo de interrupciones, incluyendo el uso de la pila.</li>
                    <li>Interrupciones anidadas y secuenciales.</li>
                    <li>Propiedades.</li>
                    <li>Rutinas de Servicio (ISR), su localización y acceso a ellas.</li>
                </ul>
                <br>

                <!-- PIPELINE -->
                <h2 class="titulo2">PIPELINE</h2>
                <b class="negrita-color">AÑO 2023 | 2024</b>
                <ul class="lista">
                    <li>En que consiste y cual es el beneficio que brinda.</li>
                    <li>Que instrucciones generan problemas, por qué, y que soluciones utiliza.</li>
                    <li>Grafique un pipeline genérico de cinco etapas de una máquina CISC y describa la función de cada una de ellas.</li>
                    <li>Grafique un diagrama de tiempo indicando la ejecución de instrucciones en ese pipeline de 5 etapas.</li>
                </ul>
                <br>

                <!-- SECUENCIADORES -->
                <h2 class="titulo2">SECUENCIADORES</h2>
                <b class="negrita-color">AÑO 2023</b>
                <ul class="lista">
                    <li>¿Para qué arquitecturas son adecuados los secuenciadores micrprogramados?</li>
                    <li>¿Para qué arquitecturas son adecuados los secuenciadores cableados?</li>
                </ul>
                <b class="negrita-color">AÑO 2024</b>
                <ul class="lista">
                    <li>Describa los secuenciadores de tipo cableados en detalles.</li>
                    <li>Describa los secuenciadores microprogramados.</li>
                    <li>Modelo de Wilkes.</li>
                    <li>Direccionamiento implícito y explícito.</li>
                    <li>Microprogramación vertical y horizontal.</li>
                    <li>Microinstrucciones por micro orden o por campo.</li>
                </ul>

                <!-- PARALELISMO -->
                <h2 class="titulo2">PARALELISMO</h2>
                <b class="negrita-color">AÑO 2023 | 2024</b>
                <ul class="lista">
                    <li>Describa la clasificación de Flynn</li>
                    <li>Respecto a MIMD, describa en detalle multicomputadores y multiprocesadores</li>
                    <li>Caracteristicas</li>
                    <li>Diferencias</li>
                </ul>

                <!-- MEMORIA CACHÉ -->
                <h2 class="titulo2">MEMORIA CACHE</h2>
                <b class="negrita-color">AÑO 2024</b>
                <ul class="lista">
                    <li>Describa el objetivo de las memorias caché y como se implementan.</li>
                    <li>Describa su funcionamiento, funciones de mapeo y algoritmos de reemplazo.</li>
                </ul>








                <button onclick="scrollToTop('examenes')">▲</button>
                <!--<button onclick="goBack()">Volver Atrás</button>-->
            </section>

            <div id="content-display" class="contenido">Selecciona una materia para ver su contenido aquí.</div>
        </main>

    </div>

    <script src="scriptM.js"></script>
</body>
</html>
